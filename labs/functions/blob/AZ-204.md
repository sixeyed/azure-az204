# Azure Functions - Blob Triggers - AZ-204 Exam Exercises

**AZ-204 Exam Domain:** Develop Azure Compute Solutions (25-30%)

This lab covers Blob-triggered Azure Functions with specific scenarios and skills required for the AZ-204 exam.

## Prerequisites

Complete the basic [Azure Functions Blob lab](README.md) first to understand fundamental blob trigger operations.

## AZ-204 Exam Skills Covered

- Implement Blob triggers with path patterns
- Configure blob input and output bindings
- Handle blob metadata and properties
- Implement poison blob handling
- Work with blob storage events
- Optimize blob trigger performance
- Best practices for blob-triggered functions

## Exercise 1: Blob Trigger Path Patterns

**AZ-204 Critical Topic:** Understanding blob trigger path patterns is essential for filtering and routing blob events.

### Basic Blob Trigger

```csharp
using Microsoft.Azure.WebJobs;
using Microsoft.Extensions.Logging;
using System.IO;

public static class BasicBlobTrigger
{
    [FunctionName("ProcessUploadedBlob")]
    public static void Run(
        [BlobTrigger("uploads/{name}", Connection = "AzureWebJobsStorage")] Stream myBlob,
        string name,
        ILogger log)
    {
        log.LogInformation($"Processing blob\n Name:{name} \n Size: {myBlob.Length} bytes");
    }
}
```

### Path Pattern Components

```csharp
public static class PathPatterns
{
    // Match all blobs in container
    [FunctionName("AllBlobs")]
    public static void ProcessAllBlobs(
        [BlobTrigger("container/{name}")] Stream blob,
        string name,
        ILogger log)
    {
        log.LogInformation($"Any blob: {name}");
    }

    // Match blobs in subfolder
    [FunctionName("SubfolderBlobs")]
    public static void ProcessSubfolder(
        [BlobTrigger("container/images/{name}")] Stream blob,
        string name,
        ILogger log)
    {
        log.LogInformation($"Image blob: {name}");
    }

    // Match blobs in any subfolder
    [FunctionName("AnySubfolderBlobs")]
    public static void ProcessAnySubfolder(
        [BlobTrigger("container/{folder}/{name}")] Stream blob,
        string folder,
        string name,
        ILogger log)
    {
        log.LogInformation($"Blob in {folder}: {name}");
    }

    // Match nested folders
    [FunctionName("NestedFolderBlobs")]
    public static void ProcessNested(
        [BlobTrigger("container/{folder1}/{folder2}/{name}")] Stream blob,
        string folder1,
        string folder2,
        string name,
        ILogger log)
    {
        log.LogInformation($"Nested blob: {folder1}/{folder2}/{name}");
    }

    // Match by file extension
    [FunctionName("ImageBlobs")]
    public static void ProcessImages(
        [BlobTrigger("uploads/{name}.{extension}")] Stream blob,
        string name,
        string extension,
        ILogger log)
    {
        log.LogInformation($"File: {name}, Extension: {extension}");

        if (extension.ToLower() == "jpg" || extension.ToLower() == "png")
        {
            log.LogInformation("Processing image file");
        }
    }
}
```

### Advanced Path Pattern Examples

```csharp
public static class AdvancedPatterns
{
    // Date-based folder structure: container/2024/01/15/file.txt
    [FunctionName("DateBasedBlobs")]
    public static void ProcessDateBased(
        [BlobTrigger("logs/{year}/{month}/{day}/{name}")] Stream blob,
        string year,
        string month,
        string day,
        string name,
        ILogger log)
    {
        log.LogInformation($"Log file from {year}-{month}-{day}: {name}");
    }

    // Customer-based structure: container/customer-123/uploads/file.pdf
    [FunctionName("CustomerBlobs")]
    public static void ProcessCustomerUploads(
        [BlobTrigger("data/{customerId}/uploads/{fileName}")] Stream blob,
        string customerId,
        string fileName,
        ILogger log)
    {
        log.LogInformation($"Processing upload for customer {customerId}: {fileName}");
    }

    // Environment-based: container/production/data/file.json
    [FunctionName("EnvironmentBlobs")]
    public static void ProcessEnvironmentData(
        [BlobTrigger("data/{environment}/{category}/{name}")] Stream blob,
        string environment,
        string category,
        string name,
        ILogger log)
    {
        log.LogInformation($"Processing {category} data from {environment}: {name}");
    }
}
```

ðŸ“‹ What path pattern would match all JSON files in any subfolder of the "documents" container?

<details>
  <summary>Not sure how?</summary>

```csharp
[BlobTrigger("documents/{folder}/{name}.json")]
```

Or for multiple levels of nesting:

```csharp
[BlobTrigger("documents/{*path}.json")]
```

The `{*path}` pattern captures the entire path including all subfolders.

</details><br/>

> **AZ-204 Exam Tip:** Path patterns support:
> - `{name}` - Captures the blob name
> - `{folder}` - Captures folder name at specific level
> - `{*path}` - Captures entire path including subfolders
> - `{name}.{extension}` - Splits name and extension

## Exercise 2: Blob Input and Output Bindings

**AZ-204 Critical Topic:** Bindings provide declarative access to blob storage without SDK code.

### Blob Input Binding

```csharp
using Microsoft.AspNetCore.Mvc;
using Microsoft.Azure.WebJobs;
using Microsoft.Azure.WebJobs.Extensions.Http;
using Microsoft.AspNetCore.Http;
using System.IO;

public static class BlobInputBinding
{
    [FunctionName("ReadBlobContent")]
    public static IActionResult Run(
        [HttpTrigger(AuthorizationLevel.Function, "get")] HttpRequest req,
        [Blob("container/{Query.filename}", FileAccess.Read, Connection = "AzureWebJobsStorage")] Stream blobContent,
        ILogger log)
    {
        log.LogInformation("Reading blob content");

        using (var reader = new StreamReader(blobContent))
        {
            string content = reader.ReadToEnd();
            return new OkObjectResult(content);
        }
    }
}
```

### Multiple Blob Bindings

```csharp
public static class MultipleBlobBindings
{
    [FunctionName("TransformBlob")]
    public static void Run(
        [BlobTrigger("input/{name}", Connection = "AzureWebJobsStorage")] string inputBlob,
        [Blob("output/{name}", FileAccess.Write, Connection = "AzureWebJobsStorage")] out string outputBlob,
        [Blob("archive/{name}", FileAccess.Write, Connection = "AzureWebJobsStorage")] out string archiveBlob,
        string name,
        ILogger log)
    {
        log.LogInformation($"Transforming blob: {name}");

        // Transform the content
        string transformed = inputBlob.ToUpper();

        // Write to output
        outputBlob = transformed;

        // Write to archive
        archiveBlob = $"Archived on {DateTime.UtcNow}: {inputBlob}";

        log.LogInformation("Blob transformed and archived");
    }
}
```

### CloudBlockBlob Binding for Metadata

```csharp
using Microsoft.Azure.Storage.Blob;
using System.Threading.Tasks;

public static class BlobWithMetadata
{
    [FunctionName("ProcessBlobWithMetadata")]
    public static async Task Run(
        [BlobTrigger("uploads/{name}", Connection = "AzureWebJobsStorage")] CloudBlockBlob blob,
        string name,
        ILogger log)
    {
        log.LogInformation($"Processing blob: {name}");

        // Fetch blob attributes (including metadata)
        await blob.FetchAttributesAsync();

        // Read metadata
        if (blob.Metadata.ContainsKey("Department"))
        {
            log.LogInformation($"Department: {blob.Metadata["Department"]}");
        }

        // Read properties
        log.LogInformation($"Content Type: {blob.Properties.ContentType}");
        log.LogInformation($"Length: {blob.Properties.Length} bytes");
        log.LogInformation($"Last Modified: {blob.Properties.LastModified}");
        log.LogInformation($"ETag: {blob.Properties.ETag}");

        // Add metadata
        blob.Metadata["ProcessedDate"] = DateTime.UtcNow.ToString("o");
        blob.Metadata["ProcessedBy"] = "BlobFunction";
        await blob.SetMetadataAsync();

        log.LogInformation("Metadata updated");
    }
}
```

### BlobClient Binding (Azure.Storage.Blobs SDK)

```csharp
using Azure.Storage.Blobs;
using Azure.Storage.Blobs.Models;
using System.Threading.Tasks;

public static class ModernBlobBinding
{
    [FunctionName("ProcessWithBlobClient")]
    public static async Task Run(
        [BlobTrigger("uploads/{name}", Connection = "AzureWebJobsStorage")] BlobClient blobClient,
        string name,
        ILogger log)
    {
        log.LogInformation($"Processing blob: {name}");

        // Get properties
        BlobProperties properties = await blobClient.GetPropertiesAsync();
        log.LogInformation($"Content Type: {properties.ContentType}");
        log.LogInformation($"Size: {properties.ContentLength} bytes");

        // Get metadata
        var metadata = properties.Metadata;
        foreach (var kvp in metadata)
        {
            log.LogInformation($"Metadata - {kvp.Key}: {kvp.Value}");
        }

        // Download content
        var downloadResult = await blobClient.DownloadContentAsync();
        string content = downloadResult.Value.Content.ToString();
        log.LogInformation($"Content preview: {content.Substring(0, Math.Min(100, content.Length))}");

        // Set metadata
        var newMetadata = new Dictionary<string, string>
        {
            { "ProcessedTimestamp", DateTime.UtcNow.ToString("o") },
            { "ProcessedBy", "AzureFunction" }
        };
        await blobClient.SetMetadataAsync(newMetadata);
    }
}
```

> **AZ-204 Exam Tip:** Blob binding types:
> - `Stream` - Read/write blob content
> - `string` - Read blob as text
> - `byte[]` - Read blob as binary
> - `CloudBlockBlob` - Full blob operations (legacy SDK)
> - `BlobClient` - Full blob operations (modern SDK)
> - `out` parameters for output bindings

## Exercise 3: Image Processing with Blob Triggers

**AZ-204 Real-World Scenario:** Process images to create thumbnails.

### Image Thumbnail Generator

```csharp
using Microsoft.Azure.WebJobs;
using Microsoft.Extensions.Logging;
using SixLabors.ImageSharp;
using SixLabors.ImageSharp.Processing;
using System.IO;
using System.Threading.Tasks;

public static class ImageProcessor
{
    [FunctionName("CreateThumbnail")]
    public static async Task Run(
        [BlobTrigger("images/original/{name}", Connection = "AzureWebJobsStorage")] Stream imageBlob,
        [Blob("images/thumbnails/{name}", FileAccess.Write, Connection = "AzureWebJobsStorage")] Stream thumbnailBlob,
        string name,
        ILogger log)
    {
        log.LogInformation($"Creating thumbnail for: {name}");

        // Check file extension
        string extension = Path.GetExtension(name).ToLower();
        if (extension != ".jpg" && extension != ".png" && extension != ".jpeg")
        {
            log.LogWarning($"Skipping non-image file: {name}");
            return;
        }

        try
        {
            using (var image = await Image.LoadAsync(imageBlob))
            {
                // Resize to thumbnail
                image.Mutate(x => x.Resize(new ResizeOptions
                {
                    Size = new Size(200, 200),
                    Mode = ResizeMode.Max
                }));

                // Save as JPEG
                await image.SaveAsJpegAsync(thumbnailBlob);
            }

            log.LogInformation($"Thumbnail created successfully for: {name}");
        }
        catch (Exception ex)
        {
            log.LogError(ex, $"Error creating thumbnail for: {name}");
            throw;
        }
    }
}
```

### Multi-Size Image Processor

```csharp
public static class MultiSizeImageProcessor
{
    [FunctionName("CreateMultipleSizes")]
    public static async Task Run(
        [BlobTrigger("uploads/images/{name}", Connection = "AzureWebJobsStorage")] Stream imageBlob,
        [Blob("processed/small/{name}", FileAccess.Write)] Stream smallBlob,
        [Blob("processed/medium/{name}", FileAccess.Write)] Stream mediumBlob,
        [Blob("processed/large/{name}", FileAccess.Write)] Stream largeBlob,
        string name,
        ILogger log)
    {
        log.LogInformation($"Processing image: {name}");

        using (var image = await Image.LoadAsync(imageBlob))
        {
            // Small: 200x200
            var small = image.Clone(x => x.Resize(200, 200));
            await small.SaveAsJpegAsync(smallBlob);

            // Medium: 800x600
            var medium = image.Clone(x => x.Resize(800, 600));
            await medium.SaveAsJpegAsync(mediumBlob);

            // Large: 1920x1080
            var large = image.Clone(x => x.Resize(1920, 1080));
            await large.SaveAsJpegAsync(largeBlob);

            log.LogInformation($"Created 3 sizes for: {name}");
        }
    }
}
```

## Exercise 4: Blob Metadata and Properties

**AZ-204 Advanced Topic:** Working with blob metadata for categorization and routing.

### Metadata-Based Processing

```csharp
using Azure.Storage.Blobs;
using Azure.Storage.Blobs.Models;
using System.Collections.Generic;
using System.Threading.Tasks;

public static class MetadataProcessor
{
    [FunctionName("ProcessByMetadata")]
    public static async Task Run(
        [BlobTrigger("documents/{name}", Connection = "AzureWebJobsStorage")] BlobClient blob,
        [Queue("high-priority", Connection = "AzureWebJobsStorage")] IAsyncCollector<string> highPriorityQueue,
        [Queue("normal-priority", Connection = "AzureWebJobsStorage")] IAsyncCollector<string> normalPriorityQueue,
        string name,
        ILogger log)
    {
        log.LogInformation($"Processing document: {name}");

        // Get metadata
        BlobProperties properties = await blob.GetPropertiesAsync();
        var metadata = properties.Metadata;

        // Route based on priority metadata
        if (metadata.ContainsKey("Priority") && metadata["Priority"] == "High")
        {
            log.LogInformation("Routing to high-priority queue");
            await highPriorityQueue.AddAsync(name);
        }
        else
        {
            log.LogInformation("Routing to normal-priority queue");
            await normalPriorityQueue.AddAsync(name);
        }

        // Add processing metadata
        var newMetadata = new Dictionary<string, string>(metadata)
        {
            { "ProcessedDate", DateTime.UtcNow.ToString("o") },
            { "ProcessedBy", "MetadataProcessor" }
        };
        await blob.SetMetadataAsync(newMetadata);
    }
}
```

### Set Content Type Based on Extension

```csharp
public static class ContentTypeUpdater
{
    [FunctionName("UpdateContentType")]
    public static async Task Run(
        [BlobTrigger("uploads/{name}", Connection = "AzureWebJobsStorage")] BlobClient blob,
        string name,
        ILogger log)
    {
        log.LogInformation($"Checking content type for: {name}");

        var properties = await blob.GetPropertiesAsync();

        string correctContentType = GetContentType(name);

        if (properties.Value.ContentType != correctContentType)
        {
            log.LogInformation($"Updating content type from '{properties.Value.ContentType}' to '{correctContentType}'");

            var headers = new BlobHttpHeaders
            {
                ContentType = correctContentType
            };

            await blob.SetHttpHeadersAsync(headers);
        }
    }

    private static string GetContentType(string fileName)
    {
        string extension = Path.GetExtension(fileName).ToLower();
        return extension switch
        {
            ".jpg" or ".jpeg" => "image/jpeg",
            ".png" => "image/png",
            ".pdf" => "application/pdf",
            ".json" => "application/json",
            ".xml" => "application/xml",
            ".txt" => "text/plain",
            ".csv" => "text/csv",
            ".html" => "text/html",
            _ => "application/octet-stream"
        };
    }
}
```

## Exercise 5: Poison Blob Handling

**AZ-204 Critical Topic:** Handle blobs that consistently fail processing.

### Poison Blob Pattern

```csharp
using Microsoft.Azure.WebJobs;
using Microsoft.Extensions.Logging;
using System;
using System.IO;
using System.Threading.Tasks;

public static class PoisonBlobHandler
{
    private const int MaxDequeueCount = 5;

    [FunctionName("ProcessWithPoisonHandling")]
    [FixedDelayRetry(5, "00:00:10")]
    public static async Task Run(
        [BlobTrigger("input/{name}", Connection = "AzureWebJobsStorage")] Stream blob,
        [Blob("poison/{name}", FileAccess.Write, Connection = "AzureWebJobsStorage")] Stream poisonBlob,
        [Blob("processed/{name}", FileAccess.Write, Connection = "AzureWebJobsStorage")] Stream processedBlob,
        string name,
        ExecutionContext context,
        ILogger log)
    {
        log.LogInformation($"Processing blob: {name}");

        try
        {
            // Attempt to process the blob
            await ProcessBlobAsync(blob, processedBlob, log);
            log.LogInformation($"Successfully processed: {name}");
        }
        catch (Exception ex)
        {
            log.LogError(ex, $"Failed to process blob: {name}");

            // After retries are exhausted, move to poison container
            if (context.RetryContext?.RetryCount >= MaxDequeueCount - 1)
            {
                log.LogWarning($"Moving blob to poison container: {name}");
                blob.Position = 0;
                await blob.CopyToAsync(poisonBlob);
            }

            throw; // Trigger retry
        }
    }

    private static async Task ProcessBlobAsync(Stream input, Stream output, ILogger log)
    {
        // Simulate processing that might fail
        using (var reader = new StreamReader(input))
        using (var writer = new StreamWriter(output))
        {
            string content = await reader.ReadToEndAsync();

            // Validate content
            if (string.IsNullOrWhiteSpace(content))
            {
                throw new InvalidDataException("Blob content is empty");
            }

            // Process content
            string processed = content.ToUpper();
            await writer.WriteAsync(processed);
        }
    }
}
```

### Poison Queue Integration

```csharp
public static class PoisonBlobWithQueue
{
    [FunctionName("ProcessBlobWithPoisonQueue")]
    public static async Task Run(
        [BlobTrigger("data/{name}", Connection = "AzureWebJobsStorage")] Stream blob,
        [Queue("poison-blobs", Connection = "AzureWebJobsStorage")] IAsyncCollector<PoisonBlobMessage> poisonQueue,
        string name,
        ILogger log)
    {
        try
        {
            await ProcessDataBlobAsync(blob, log);
        }
        catch (Exception ex)
        {
            log.LogError(ex, $"Error processing blob: {name}");

            // Send to poison queue for manual review
            await poisonQueue.AddAsync(new PoisonBlobMessage
            {
                BlobName = name,
                ErrorMessage = ex.Message,
                Timestamp = DateTime.UtcNow
            });

            throw;
        }
    }

    private static async Task ProcessDataBlobAsync(Stream blob, ILogger log)
    {
        // Processing logic
        await Task.Delay(100);
    }
}

public class PoisonBlobMessage
{
    public string BlobName { get; set; }
    public string ErrorMessage { get; set; }
    public DateTime Timestamp { get; set; }
}
```

> **AZ-204 Exam Tip:** Poison blob handling strategies:
> - Use retry policies with limited attempts
> - Move failed blobs to poison container
> - Send alerts via Queue/Service Bus
> - Log detailed error information
> - Implement manual review process

## Exercise 6: Blob Storage Events Integration

**AZ-204 Advanced Topic:** Using Event Grid for more reliable blob event processing.

### Event Grid Trigger for Blobs

```csharp
using Microsoft.Azure.WebJobs;
using Microsoft.Azure.WebJobs.Extensions.EventGrid;
using Azure.Messaging.EventGrid;
using Microsoft.Extensions.Logging;
using System.Threading.Tasks;

public static class EventGridBlobProcessor
{
    [FunctionName("BlobEventGridTrigger")]
    public static async Task Run(
        [EventGridTrigger] EventGridEvent eventGridEvent,
        [Blob("{data.url}", FileAccess.Read, Connection = "AzureWebJobsStorage")] Stream blob,
        ILogger log)
    {
        log.LogInformation($"Event type: {eventGridEvent.EventType}");
        log.LogInformation($"Event subject: {eventGridEvent.Subject}");

        // Parse blob created event
        if (eventGridEvent.EventType == "Microsoft.Storage.BlobCreated")
        {
            log.LogInformation("Processing newly created blob");

            // Get blob URL from event data
            var data = eventGridEvent.Data.ToObjectFromJson<BlobCreatedEventData>();
            log.LogInformation($"Blob URL: {data.Url}");
            log.LogInformation($"Blob size: {data.ContentLength} bytes");
            log.LogInformation($"Blob type: {data.BlobType}");

            // Process the blob
            await ProcessBlobFromEventAsync(blob, log);
        }
    }

    private static async Task ProcessBlobFromEventAsync(Stream blob, ILogger log)
    {
        log.LogInformation("Processing blob content from Event Grid event");
        // Processing logic
        await Task.CompletedTask;
    }
}

public class BlobCreatedEventData
{
    public string Api { get; set; }
    public string ClientRequestId { get; set; }
    public string RequestId { get; set; }
    public string ETag { get; set; }
    public string ContentType { get; set; }
    public long ContentLength { get; set; }
    public string BlobType { get; set; }
    public string Url { get; set; }
    public string Sequencer { get; set; }
}
```

### Blob Trigger vs Event Grid

| Feature | Blob Trigger | Event Grid Trigger |
|---------|--------------|-------------------|
| Latency | Minutes (polling) | Seconds (push) |
| Reliability | Can miss events | Guaranteed delivery |
| Filtering | Path patterns | Advanced filters |
| Cost | Included | Per-event pricing |
| Scale | Limited | High scale |
| Use case | Simple scenarios | Production systems |

> **AZ-204 Exam Tip:** Event Grid provides:
> - Near real-time blob event processing
> - Guaranteed event delivery with retry
> - Advanced filtering capabilities
> - Better than blob triggers for production workloads

## Exercise 7: Performance Optimization

### Batch Processing with Blob Triggers

```csharp
using System.Collections.Concurrent;
using System.Threading.Tasks;

public static class BatchBlobProcessor
{
    private static readonly ConcurrentBag<string> BatchQueue = new ConcurrentBag<string>();
    private const int BatchSize = 10;

    [FunctionName("BatchProcessor")]
    public static async Task Run(
        [BlobTrigger("input/{name}", Connection = "AzureWebJobsStorage")] Stream blob,
        [Blob("output/batch-{DateTime}.json", FileAccess.Write)] Stream outputBlob,
        string name,
        ILogger log)
    {
        log.LogInformation($"Adding to batch: {name}");

        // Add to batch
        BatchQueue.Add(name);

        // Process when batch is full
        if (BatchQueue.Count >= BatchSize)
        {
            log.LogInformation($"Processing batch of {BatchQueue.Count} items");

            var batch = new List<string>();
            while (BatchQueue.TryTake(out string item))
            {
                batch.Add(item);
            }

            await ProcessBatchAsync(batch, outputBlob, log);
        }
    }

    private static async Task ProcessBatchAsync(List<string> batch, Stream output, ILogger log)
    {
        log.LogInformation($"Processing {batch.Count} blobs in batch");
        // Batch processing logic
        await Task.Delay(100);
    }
}
```

### Async Blob Processing

```csharp
public static class AsyncBlobProcessor
{
    [FunctionName("AsyncBlobProcessor")]
    public static async Task Run(
        [BlobTrigger("uploads/{name}", Connection = "AzureWebJobsStorage")] Stream blob,
        [Queue("processing-tasks", Connection = "AzureWebJobsStorage")] IAsyncCollector<ProcessingTask> taskQueue,
        string name,
        ILogger log)
    {
        log.LogInformation($"Received blob: {name}");

        // Queue for async processing instead of processing synchronously
        await taskQueue.AddAsync(new ProcessingTask
        {
            BlobName = name,
            QueuedAt = DateTime.UtcNow
        });

        log.LogInformation($"Queued for processing: {name}");
    }

    [FunctionName("ProcessTask")]
    public static async Task ProcessQueuedTask(
        [QueueTrigger("processing-tasks", Connection = "AzureWebJobsStorage")] ProcessingTask task,
        [Blob("uploads/{BlobName}", FileAccess.Read, Connection = "AzureWebJobsStorage")] Stream blob,
        ILogger log)
    {
        log.LogInformation($"Processing task: {task.BlobName}");

        // Perform intensive processing
        await Task.Delay(5000); // Simulate heavy processing

        log.LogInformation($"Completed processing: {task.BlobName}");
    }
}

public class ProcessingTask
{
    public string BlobName { get; set; }
    public DateTime QueuedAt { get; set; }
}
```

## AZ-204 Exam Study Points

### Key Concepts to Master

1. **Blob Trigger Path Patterns:**
   - `{name}` - Blob name variable
   - `{folder}/{name}` - Specific folder level
   - `{*path}` - Entire path including subfolders
   - `{name}.{extension}` - Split name and extension
   - Multiple variable extraction from path

2. **Blob Binding Types:**
   - **Stream**: Read/write operations
   - **string**: Text content
   - **byte[]**: Binary content
   - **CloudBlockBlob**: Legacy SDK (full operations)
   - **BlobClient**: Modern SDK (full operations)
   - Direction: In (read), Out (write), InOut (both)

3. **Blob Metadata vs Properties:**
   - **Properties**: System-defined (ContentType, ETag, Length)
   - **Metadata**: Custom key-value pairs
   - Set/read via BlobClient or CloudBlockBlob
   - Used for categorization and routing

4. **Poison Blob Handling:**
   - Retry policies: FixedDelayRetry, ExponentialBackoffRetry
   - Move to poison container after max retries
   - Log failures for investigation
   - Alert via queues or monitoring

5. **Event Grid Integration:**
   - Real-time blob events (seconds vs minutes)
   - Guaranteed delivery
   - Advanced filtering
   - BlobCreated and BlobDeleted events

6. **Performance Considerations:**
   - Blob triggers use polling (delay up to 10 seconds)
   - Event Grid for near real-time processing
   - Batch processing for efficiency
   - Async processing via queues

### Common Exam Scenarios

1. **Scenario:** Process all images uploaded to a specific folder
   - **Solution:** `[BlobTrigger("images/{name}")]` with extension checking

2. **Scenario:** Create thumbnails when images are uploaded
   - **Solution:** Blob trigger with input binding, image processing library, output binding for thumbnail

3. **Scenario:** Route blobs based on metadata priority
   - **Solution:** Read metadata with BlobClient, use conditional logic with queue output bindings

4. **Scenario:** Handle blobs that consistently fail processing
   - **Solution:** Retry policy + move to poison container after max attempts

5. **Scenario:** Process blobs in near real-time
   - **Solution:** Use Event Grid trigger instead of blob trigger

6. **Scenario:** Extract customer ID from path like "uploads/customer-123/file.pdf"
   - **Solution:** `[BlobTrigger("uploads/{customerId}/{name}")]`

7. **Scenario:** Update blob content type based on file extension
   - **Solution:** BlobClient binding with SetHttpHeadersAsync()

### Blob Trigger Limitations

1. **Polling Delay:** 10+ seconds for blob detection (use Event Grid for real-time)
2. **Scale Limits:** One blob trigger instance per container per host
3. **Missing Events:** Possible during outages (Event Grid is more reliable)
4. **Blob Metadata:** Not available with Stream binding (use BlobClient)
5. **Large Blobs:** Consider streaming or chunked processing

### Connection String Configuration

```json
{
  "IsEncrypted": false,
  "Values": {
    "AzureWebJobsStorage": "DefaultEndpointsProtocol=https;AccountName=...",
    "FUNCTIONS_WORKER_RUNTIME": "dotnet",
    "StorageConnection": "DefaultEndpointsProtocol=https;AccountName=..."
  }
}
```

Reference in bindings:
```csharp
[BlobTrigger("container/{name}", Connection = "StorageConnection")]
```

## Best Practices

1. **Use Event Grid** for production blob processing (more reliable)
2. **Validate blob content** before processing
3. **Implement retry policies** with poison blob handling
4. **Use appropriate binding types** (Stream for large files, string for small text)
5. **Set correct content types** for proper rendering
6. **Add metadata** for categorization and tracking
7. **Monitor Application Insights** for processing metrics
8. **Use path patterns** to filter relevant blobs
9. **Consider batch processing** for high-volume scenarios
10. **Test with various blob sizes** and error conditions

## Additional Resources

- [Azure Functions Blob trigger documentation](https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-trigger)
- [Blob trigger vs Event Grid](https://docs.microsoft.com/en-us/azure/azure-functions/functions-event-grid-blob-trigger)
- [Azure.Storage.Blobs SDK](https://docs.microsoft.com/en-us/dotnet/api/overview/azure/storage.blobs-readme)
- [AZ-204 Study Guide](https://learn.microsoft.com/en-us/credentials/certifications/resources/study-guides/az-204)

## Cleanup

```bash
az group delete -y -n labs-functions-blob --no-wait
```
