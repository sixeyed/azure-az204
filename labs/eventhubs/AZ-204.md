# Event Hubs - AZ-204 Exam Exercises

**AZ-204 Exam Domain:** Develop message-based solutions (10-15%)

This lab extends the basic Event Hubs exercises with specific scenarios and skills required for the AZ-204 exam.

## Prerequisites

Complete the basic [Event Hubs lab](README.md) first to understand fundamental event streaming operations.

## AZ-204 Exam Skills Covered

- Understand Event Hubs concepts (partitions, consumer groups, checkpointing)
- Compare Event Hubs vs Event Grid vs Service Bus
- Send and receive events using the SDK
- Configure Event Hubs Capture to storage
- Manage throughput units and auto-inflate
- Implement Event Processor Host/SDK for scalable processing
- Understand checkpointing and partition ownership

## Exercise 1: Event Hubs Core Concepts

**AZ-204 Critical Topic:** Understanding Event Hubs architecture and terminology.

### Key Concepts

**Event Hubs Namespace:**
- Container for multiple event hubs
- Provides DNS-integrated network endpoint
- Manages authentication and authorization

**Event Hub:**
- Ordered sequence of events (append-only log)
- Events retained based on retention policy
- Divided into partitions for scalability

**Partitions:**
- Ordered sequence within event hub
- Number set at creation (1-32 or more)
- Events distributed by partition key or round-robin
- Enable parallel processing

**Consumer Groups:**
- View (state/position) of entire event hub
- Each consumer group reads independently
- Default consumer group: `$Default`
- Up to 20 consumer groups per event hub

**Offset:**
- Position of event within partition
- Consumers track their progress via offset
- Stored as checkpoint

### Create Event Hubs Namespace and Event Hub

```bash
az group create -n labs-eventhubs-az204 --tags courselabs=azure -l eastus

# Create Event Hubs namespace
EVENTHUB_NAMESPACE=evhns$RANDOM
az eventhubs namespace create \
  -g labs-eventhubs-az204 \
  -n $EVENTHUB_NAMESPACE \
  -l eastus \
  --sku Standard

# Create event hub with 4 partitions
az eventhubs eventhub create \
  -g labs-eventhubs-az204 \
  --namespace-name $EVENTHUB_NAMESPACE \
  -n telemetry \
  --partition-count 4 \
  --message-retention 1
```

### View Event Hub Details

```bash
az eventhubs eventhub show \
  -g labs-eventhubs-az204 \
  --namespace-name $EVENTHUB_NAMESPACE \
  -n telemetry \
  --query '{name:name, partitionCount:partitionCount, status:status, messageRetention:messageRetentionInDays}' \
  -o table
```

### Create Consumer Groups

```bash
# Create consumer group for analytics
az eventhubs eventhub consumer-group create \
  -g labs-eventhubs-az204 \
  --namespace-name $EVENTHUB_NAMESPACE \
  --eventhub-name telemetry \
  -n analytics

# Create consumer group for archival
az eventhubs eventhub consumer-group create \
  -g labs-eventhubs-az204 \
  --namespace-name $EVENTHUB_NAMESPACE \
  --eventhub-name telemetry \
  -n archival

# List consumer groups
az eventhubs eventhub consumer-group list \
  -g labs-eventhubs-az204 \
  --namespace-name $EVENTHUB_NAMESPACE \
  --eventhub-name telemetry \
  -o table
```

ðŸ“‹ Create an event hub with 8 partitions and 7-day retention, then add two custom consumer groups.

<details>
  <summary>Not sure how?</summary>

```bash
az eventhubs eventhub create \
  -g labs-eventhubs-az204 \
  --namespace-name $EVENTHUB_NAMESPACE \
  -n orders \
  --partition-count 8 \
  --message-retention 7

az eventhubs eventhub consumer-group create \
  -g labs-eventhubs-az204 \
  --namespace-name $EVENTHUB_NAMESPACE \
  --eventhub-name orders \
  -n processing

az eventhubs eventhub consumer-group create \
  -g labs-eventhubs-az204 \
  --namespace-name $EVENTHUB_NAMESPACE \
  --eventhub-name orders \
  -n monitoring
```

</details><br/>

> **AZ-204 Exam Tip:**
> - **Partitions**: Cannot change after creation, choose carefully
> - **Retention**: 1-7 days (Standard), up to 90 days (Premium/Dedicated)
> - **Consumer Groups**: Enable multiple independent consumers
> - Each consumer group maintains separate offset tracking
> - $Default consumer group automatically created

## Exercise 2: Event Hubs vs Event Grid vs Service Bus

**AZ-204 Critical:** Understand when to use each Azure messaging service.

### Comparison Matrix

| Feature | Event Hubs | Event Grid | Service Bus |
|---------|------------|------------|-------------|
| **Pattern** | Event Streaming | Event Distribution | Message Queuing |
| **Message Size** | Up to 1 MB | 1 MB | 256 KB (Standard), 100 MB (Premium) |
| **Throughput** | Millions/sec | Millions/sec | Thousands/sec |
| **Ordering** | Per partition | No guarantee | Sessions/FIFO |
| **Retention** | 1-90 days | 24 hours retry | 14 days (max lock) |
| **Consumption** | Pull (consumer) | Push (webhook) | Pull (receiver) |
| **Delivery** | At-least-once | At-least-once | At-most-once/At-least-once |
| **Routing** | Partition key | Event subscriptions | Topics/subscriptions |
| **Use Case** | Telemetry, Logs | Reactive, Events | Commands, Transactions |

### When to Use Event Hubs

**Best For:**
- **Big data streaming**: IoT telemetry, application logs
- **High throughput**: Millions of events per second
- **Multiple consumers**: Different systems reading same events
- **Replay capability**: Historical data analysis
- **Real-time analytics**: Stream processing, anomaly detection

**Examples:**
```bash
# IoT device telemetry
# Log aggregation from multiple sources
# Financial transaction streams
# Gaming telemetry
# Clickstream analytics
```

### When to Use Event Grid

**Best For:**
- **Event-driven architectures**: React to Azure resource changes
- **Serverless**: Trigger Azure Functions
- **Push-based delivery**: Webhooks to endpoints
- **Filtering**: Route events based on criteria
- **Low latency**: Sub-second event delivery

**Examples:**
```bash
# Blob storage events (file uploaded)
# Resource group changes
# IoT Hub device events
# Custom application events
# Automate operations based on events
```

### When to Use Service Bus

**Best For:**
- **Enterprise messaging**: Reliable message delivery
- **Transactions**: ACID transactions
- **Message sessions**: Ordered processing
- **Duplicate detection**: Exactly-once semantics
- **Dead-lettering**: Handle poison messages

**Examples:**
```bash
# Order processing
# Payment transactions
# Command messages
# Request/response patterns
# Saga patterns (distributed transactions)
```

### Decision Tree

```
High-throughput telemetry/logs? â†’ Event Hubs
  â”œâ”€ Need replay capability? â†’ Event Hubs
  â””â”€ Need multiple consumers? â†’ Event Hubs

Reacting to Azure events? â†’ Event Grid
  â”œâ”€ Need push/webhooks? â†’ Event Grid
  â””â”€ Serverless triggers? â†’ Event Grid

Business transactions/commands? â†’ Service Bus
  â”œâ”€ Need FIFO ordering? â†’ Service Bus (sessions)
  â”œâ”€ Need transactions? â†’ Service Bus
  â””â”€ Need dead-lettering? â†’ Service Bus
```

ðŸ“‹ Choose the appropriate service for these scenarios:

<details>
  <summary>See Scenarios</summary>

**Scenario 1:** 10,000 IoT devices sending temperature readings every 5 seconds
- **Answer:** Event Hubs
- **Reason:** High throughput, multiple consumers, streaming analytics

**Scenario 2:** Send notification when blob is uploaded to storage
- **Answer:** Event Grid
- **Reason:** React to Azure events, push-based, serverless trigger

**Scenario 3:** Process customer orders with guaranteed delivery
- **Answer:** Service Bus
- **Reason:** Transactional messaging, dead-lettering, sessions for ordering

**Scenario 4:** Aggregate logs from 1000 web servers
- **Answer:** Event Hubs
- **Reason:** High volume, streaming, multiple consumers (analytics, storage)

**Scenario 5:** Trigger workflow when new user registers
- **Answer:** Event Grid
- **Reason:** Event-driven, push-based, low latency

**Scenario 6:** Payment processing requiring exactly-once semantics
- **Answer:** Service Bus with duplicate detection
- **Reason:** Transactional, reliable delivery, duplicate detection

</details><br/>

> **AZ-204 Exam Tip:** Quick decision guide:
> - **Event Hubs**: High-volume streaming (telemetry, logs)
> - **Event Grid**: Reactive programming (push notifications)
> - **Service Bus**: Reliable messaging (transactions, commands)
>
> Remember: Event Hubs = streaming, Event Grid = events, Service Bus = messages

## Exercise 3: Sending and Receiving Events with SDK

**AZ-204 Critical:** Know how to use the SDK to send and receive events.

### Get Connection String

```bash
# Get connection string for namespace
CONNECTION_STRING=$(az eventhubs namespace authorization-rule keys list \
  -g labs-eventhubs-az204 \
  --namespace-name $EVENTHUB_NAMESPACE \
  -n RootManageSharedAccessKey \
  --query primaryConnectionString -o tsv)

echo $CONNECTION_STRING
```

### Send Events (.NET SDK)

```csharp
using Azure.Messaging.EventHubs;
using Azure.Messaging.EventHubs.Producer;
using System.Text;

// Create producer client
var connectionString = "<CONNECTION_STRING>";
var eventHubName = "telemetry";

await using var producer = new EventHubProducerClient(connectionString, eventHubName);

// Send single event
var eventData = new EventData(Encoding.UTF8.GetBytes("Hello Event Hubs"));
await producer.SendAsync(new[] { eventData });

// Send batch of events
var batch = await producer.CreateBatchAsync();

for (int i = 0; i < 100; i++)
{
    var data = new EventData(Encoding.UTF8.GetBytes($"Event {i}"));

    if (!batch.TryAdd(data))
    {
        // Batch is full, send it
        await producer.SendAsync(batch);
        batch.Dispose();
        batch = await producer.CreateBatchAsync();
        batch.TryAdd(data);
    }
}

// Send remaining events
if (batch.Count > 0)
{
    await producer.SendAsync(batch);
}

// Send to specific partition
var partitionId = "0";
var batchOptions = new CreateBatchOptions { PartitionId = partitionId };
var partitionBatch = await producer.CreateBatchAsync(batchOptions);
partitionBatch.TryAdd(new EventData(Encoding.UTF8.GetBytes("Partition 0 event")));
await producer.SendAsync(partitionBatch);

// Send with partition key (same key â†’ same partition)
var keyBatchOptions = new CreateBatchOptions { PartitionKey = "device-001" };
var keyBatch = await producer.CreateBatchAsync(keyBatchOptions);
keyBatch.TryAdd(new EventData(Encoding.UTF8.GetBytes("Device 001 telemetry")));
await producer.SendAsync(keyBatch);
```

### Receive Events (.NET SDK)

```csharp
using Azure.Messaging.EventHubs;
using Azure.Messaging.EventHubs.Consumer;

// Create consumer client
var consumerGroup = EventHubConsumerClient.DefaultConsumerGroupName;
await using var consumer = new EventHubConsumerClient(
    consumerGroup,
    connectionString,
    eventHubName);

// Read from specific partition
var partitionId = "0";
await foreach (var partitionEvent in consumer.ReadEventsFromPartitionAsync(
    partitionId,
    EventPosition.Earliest))
{
    string eventBody = Encoding.UTF8.GetString(partitionEvent.Data.Body.ToArray());
    Console.WriteLine($"Event: {eventBody}");
    Console.WriteLine($"Partition: {partitionEvent.Partition.PartitionId}");
    Console.WriteLine($"Offset: {partitionEvent.Data.Offset}");
    Console.WriteLine($"Sequence: {partitionEvent.Data.SequenceNumber}");
}

// Read from all partitions
await foreach (var partitionEvent in consumer.ReadEventsAsync())
{
    // Process event
    Console.WriteLine($"Partition {partitionEvent.Partition.PartitionId}: {partitionEvent.Data.EventBody}");

    // Access properties
    foreach (var property in partitionEvent.Data.Properties)
    {
        Console.WriteLine($"  {property.Key}: {property.Value}");
    }
}
```

### Send Events with Properties

```csharp
// Add custom properties
var eventData = new EventData(Encoding.UTF8.GetBytes("Temperature: 72.5"));
eventData.Properties.Add("DeviceId", "sensor-001");
eventData.Properties.Add("Location", "Building-A");
eventData.Properties.Add("AlertLevel", "Normal");

await producer.SendAsync(new[] { eventData });
```

ðŸ“‹ Write code to send 1000 events with partition keys based on device ID.

<details>
  <summary>Not sure how?</summary>

```csharp
await using var producer = new EventHubProducerClient(connectionString, eventHubName);

// Send events for 10 devices
for (int deviceId = 1; deviceId <= 10; deviceId++)
{
    var partitionKey = $"device-{deviceId:D3}";
    var batchOptions = new CreateBatchOptions { PartitionKey = partitionKey };
    var batch = await producer.CreateBatchAsync(batchOptions);

    for (int i = 0; i < 100; i++)
    {
        var telemetry = new
        {
            DeviceId = partitionKey,
            Temperature = 70 + Random.Shared.Next(-10, 10),
            Humidity = 50 + Random.Shared.Next(-5, 5),
            Timestamp = DateTime.UtcNow
        };

        var json = JsonSerializer.Serialize(telemetry);
        var eventData = new EventData(Encoding.UTF8.GetBytes(json));

        if (!batch.TryAdd(eventData))
        {
            await producer.SendAsync(batch);
            batch.Dispose();
            batch = await producer.CreateBatchAsync(batchOptions);
            batch.TryAdd(eventData);
        }
    }

    if (batch.Count > 0)
    {
        await producer.SendAsync(batch);
    }
}

// All events from same device go to same partition
```

</details><br/>

> **AZ-204 Exam Tip:**
> - **Partition Key**: Ensures same key â†’ same partition (ordering)
> - **Partition ID**: Directly target specific partition
> - **No key/ID**: Round-robin distribution
> - Use **batches** for better throughput
> - **Properties**: Add metadata without modifying body
> - Always dispose clients properly

## Exercise 4: Event Hubs Capture

**AZ-204 Topic:** Automatically capture streaming data to storage.

### Enable Capture to Blob Storage

```bash
# Create storage account
STORAGE_ACCOUNT=evhstorage$RANDOM
az storage account create \
  -g labs-eventhubs-az204 \
  -n $STORAGE_ACCOUNT \
  --sku Standard_LRS

# Create container
az storage container create \
  -n eventhub-capture \
  --account-name $STORAGE_ACCOUNT

# Get storage account ID
STORAGE_ID=$(az storage account show \
  -n $STORAGE_ACCOUNT \
  --query id -o tsv)

# Enable capture on event hub
az eventhubs eventhub update \
  -g labs-eventhubs-az204 \
  --namespace-name $EVENTHUB_NAMESPACE \
  -n telemetry \
  --enable-capture true \
  --capture-interval 300 \
  --capture-size-limit 314572800 \
  --destination-name EventHubArchive.AzureBlockBlob \
  --storage-account $STORAGE_ID \
  --blob-container eventhub-capture \
  --archive-name-format '{Namespace}/{EventHub}/{PartitionId}/{Year}/{Month}/{Day}/{Hour}/{Minute}/{Second}'
```

### Configure Capture Settings

```bash
# Update capture window (5 minutes OR 100 MB)
az eventhubs eventhub update \
  -g labs-eventhubs-az204 \
  --namespace-name $EVENTHUB_NAMESPACE \
  -n telemetry \
  --capture-interval 300 \
  --capture-size-limit 104857600

# Update capture path format
az eventhubs eventhub update \
  -g labs-eventhubs-az204 \
  --namespace-name $EVENTHUB_NAMESPACE \
  -n telemetry \
  --archive-name-format '{Namespace}/{EventHub}/year={Year}/month={Month}/day={Day}/partition={PartitionId}'
```

### View Captured Files

```bash
# Send some events first (use SDK or Azure Portal)

# Wait for capture window to complete

# List captured files
az storage blob list \
  --account-name $STORAGE_ACCOUNT \
  --container-name eventhub-capture \
  --query '[].{name:name, size:properties.contentLength}' \
  -o table

# Download and examine captured file (Avro format)
az storage blob download \
  --account-name $STORAGE_ACCOUNT \
  --container-name eventhub-capture \
  --name '<blob-path>' \
  --file captured-events.avro
```

### Reading Avro Files

```csharp
using Avro.File;
using Avro.Generic;

// Read Avro file
using var reader = DataFileReader<GenericRecord>.OpenReader("captured-events.avro");

while (reader.HasNext())
{
    var record = reader.Next();
    var body = record["Body"] as byte[];
    var enqueuedTime = (DateTime)record["EnqueuedTimeUtc"];
    var offset = (long)record["Offset"];
    var sequenceNumber = (long)record["SequenceNumber"];

    string message = Encoding.UTF8.GetString(body);
    Console.WriteLine($"Message: {message}, Time: {enqueuedTime}");
}
```

ðŸ“‹ Configure capture to trigger every 1 minute or 10 MB, whichever comes first.

<details>
  <summary>Not sure how?</summary>

```bash
az eventhubs eventhub update \
  -g labs-eventhubs-az204 \
  --namespace-name $EVENTHUB_NAMESPACE \
  -n telemetry \
  --capture-interval 60 \
  --capture-size-limit 10485760

# Capture triggers when:
# - 60 seconds elapse, OR
# - 10 MB of data accumulated
# Whichever happens first
```

</details><br/>

> **AZ-204 Exam Tip:**
> - **Capture Window**: Time (60-900 sec) OR Size (10 MB - 500 MB)
> - **Format**: Avro (Apache Avro format)
> - **Storage**: Blob Storage or Data Lake Storage Gen2
> - **Path**: Customizable with tokens (Namespace, EventHub, Partition, Date/Time)
> - **Use Cases**: Cold storage, batch analytics, compliance
> - No impact on streaming consumers

## Exercise 5: Throughput Units and Auto-Inflate

**AZ-204 Topic:** Manage capacity and scaling.

### Understanding Throughput Units (TUs)

**Throughput Unit (Standard Tier):**
- **Ingress**: Up to 1 MB/s or 1,000 events/s
- **Egress**: Up to 2 MB/s or 4,096 events/s
- **Storage**: 84 GB included per TU

### View Current Throughput Units

```bash
az eventhubs namespace show \
  -g labs-eventhubs-az204 \
  -n $EVENTHUB_NAMESPACE \
  --query '{sku:sku.name, capacity:sku.capacity, isAutoInflateEnabled:isAutoInflateEnabled}' \
  -o json
```

### Update Throughput Units

```bash
# Scale up to 5 TUs
az eventhubs namespace update \
  -g labs-eventhubs-az204 \
  -n $EVENTHUB_NAMESPACE \
  --capacity 5

# Scale down to 2 TUs
az eventhubs namespace update \
  -g labs-eventhubs-az204 \
  -n $EVENTHUB_NAMESPACE \
  --capacity 2
```

### Enable Auto-Inflate

```bash
# Enable auto-inflate with max 20 TUs
az eventhubs namespace update \
  -g labs-eventhubs-az204 \
  -n $EVENTHUB_NAMESPACE \
  --enable-auto-inflate true \
  --maximum-throughput-units 20

# Verify auto-inflate settings
az eventhubs namespace show \
  -g labs-eventhubs-az204 \
  -n $EVENTHUB_NAMESPACE \
  --query '{current:sku.capacity, autoInflate:isAutoInflateEnabled, max:maximumThroughputUnits}' \
  -o table
```

### Monitor Throughput

```bash
# View metrics (requires Azure Monitor)
az monitor metrics list \
  --resource $(az eventhubs namespace show -g labs-eventhubs-az204 -n $EVENTHUB_NAMESPACE --query id -o tsv) \
  --metric IncomingMessages \
  --start-time 2024-01-01T00:00:00Z \
  --end-time 2024-01-01T23:59:59Z \
  --interval PT1H
```

### Processing Units (Premium Tier)

```bash
# Create Premium namespace with 1 PU
az eventhubs namespace create \
  -g labs-eventhubs-az204 \
  -n evhns-premium-$RANDOM \
  -l eastus \
  --sku Premium \
  --capacity 1

# Premium features:
# - Processing Units (PUs) instead of TUs
# - 1 PU â‰ˆ 4 TUs performance
# - CPU and memory isolation
# - Longer retention (up to 90 days)
# - Virtual network integration
# - Kafka support
```

ðŸ“‹ Configure auto-inflate to automatically scale from 2 to 10 TUs based on load.

<details>
  <summary>Not sure how?</summary>

```bash
# Set base capacity to 2 TUs
az eventhubs namespace update \
  -g labs-eventhubs-az204 \
  -n $EVENTHUB_NAMESPACE \
  --capacity 2

# Enable auto-inflate with max 10 TUs
az eventhubs namespace update \
  -g labs-eventhubs-az204 \
  -n $EVENTHUB_NAMESPACE \
  --enable-auto-inflate true \
  --maximum-throughput-units 10

# Auto-inflate will automatically scale TUs:
# - Start at 2 TUs
# - Scale up when throttled (ServerErrors metric)
# - Scale up to max 10 TUs
# - Billing based on actual TUs used
```

</details><br/>

> **AZ-204 Exam Tip:**
> - **Standard Tier**: Throughput Units (TUs), 1-20 TUs (40 with support)
> - **Premium Tier**: Processing Units (PUs), 1-16 PUs
> - **Auto-inflate**: Automatically scales TUs based on load
> - **Throttling**: Returns ServerBusy error when exceeded
> - **Billing**: Per TU/PU per hour
> - Premium offers better isolation and performance

## Exercise 6: Event Processor Host/SDK

**AZ-204 Critical:** Implement scalable event processing with automatic partition distribution.

### Understanding Event Processor

**Benefits:**
- Automatic load balancing across multiple instances
- Checkpoint management
- Partition ownership coordination
- Fault tolerance and recovery

**Components:**
- **Blob Storage**: Stores checkpoints and partition ownership
- **Consumer Group**: Isolates processing
- **Processor Instance**: Reads and processes events

### Create Event Processor (.NET)

```csharp
using Azure.Messaging.EventHubs;
using Azure.Messaging.EventHubs.Consumer;
using Azure.Messaging.EventHubs.Processor;
using Azure.Storage.Blobs;

// Connection strings
var eventHubConnectionString = "<EVENT_HUB_CONNECTION_STRING>";
var eventHubName = "telemetry";
var consumerGroup = "$Default";
var storageConnectionString = "<STORAGE_CONNECTION_STRING>";
var blobContainerName = "checkpoints";

// Create blob container client
var storageClient = new BlobContainerClient(
    storageConnectionString,
    blobContainerName);
await storageClient.CreateIfNotExistsAsync();

// Create event processor
var processor = new EventProcessorClient(
    storageClient,
    consumerGroup,
    eventHubConnectionString,
    eventHubName);

// Register handlers
processor.ProcessEventAsync += ProcessEventHandler;
processor.ProcessErrorAsync += ProcessErrorHandler;

// Start processing
await processor.StartProcessingAsync();

Console.WriteLine("Event processor started. Press Enter to stop.");
Console.ReadLine();

// Stop processing
await processor.StopProcessingAsync();

// Process event handler
async Task ProcessEventHandler(ProcessEventArgs args)
{
    try
    {
        // Process the event
        string eventBody = Encoding.UTF8.GetString(args.Data.Body.ToArray());
        Console.WriteLine($"Partition {args.Partition.PartitionId}: {eventBody}");

        // Your processing logic here
        await Task.Delay(100); // Simulate processing

        // Checkpoint after successful processing
        await args.UpdateCheckpointAsync();
    }
    catch (Exception ex)
    {
        Console.WriteLine($"Error processing event: {ex.Message}");
    }
}

// Error handler
Task ProcessErrorHandler(ProcessErrorEventArgs args)
{
    Console.WriteLine($"Error on partition {args.PartitionId}: {args.Exception.Message}");
    return Task.CompletedTask;
}
```

### Checkpointing Strategies

```csharp
// Strategy 1: Checkpoint every event (safest, slowest)
async Task ProcessEventHandler(ProcessEventArgs args)
{
    await ProcessEvent(args.Data);
    await args.UpdateCheckpointAsync();
}

// Strategy 2: Checkpoint every N events (balanced)
private int eventCount = 0;
private const int CheckpointInterval = 100;

async Task ProcessEventHandler(ProcessEventArgs args)
{
    await ProcessEvent(args.Data);

    eventCount++;
    if (eventCount >= CheckpointInterval)
    {
        await args.UpdateCheckpointAsync();
        eventCount = 0;
    }
}

// Strategy 3: Checkpoint on time interval (throughput optimized)
private DateTime lastCheckpoint = DateTime.UtcNow;
private const int CheckpointSeconds = 30;

async Task ProcessEventHandler(ProcessEventArgs args)
{
    await ProcessEvent(args.Data);

    if ((DateTime.UtcNow - lastCheckpoint).TotalSeconds >= CheckpointSeconds)
    {
        await args.UpdateCheckpointAsync();
        lastCheckpoint = DateTime.UtcNow;
    }
}

// Strategy 4: Checkpoint on batch completion
async Task ProcessEventHandler(ProcessEventArgs args)
{
    await ProcessEvent(args.Data);

    // Check if this is the last event in current batch
    if (args.HasEvent && !args.CancellationToken.IsCancellationRequested)
    {
        await args.UpdateCheckpointAsync();
    }
}
```

### Scale Out Event Processor

```csharp
// Instance 1
var processor1 = new EventProcessorClient(storageClient, consumerGroup, connectionString, eventHubName);
processor1.ProcessEventAsync += ProcessEventHandler;
processor1.ProcessErrorAsync += ProcessErrorHandler;
await processor1.StartProcessingAsync();

// Instance 2 (can be on different machine)
var processor2 = new EventProcessorClient(storageClient, consumerGroup, connectionString, eventHubName);
processor2.ProcessEventAsync += ProcessEventHandler;
processor2.ProcessErrorAsync += ProcessErrorHandler;
await processor2.StartProcessingAsync();

// Partitions automatically balanced across instances
// If you have 4 partitions:
// - Instance 1 might handle partitions 0, 1
// - Instance 2 might handle partitions 2, 3
// Rebalancing happens automatically if instance fails
```

ðŸ“‹ Implement event processor with checkpointing every 50 events and error handling.

<details>
  <summary>Not sure how?</summary>

```csharp
private int eventsSinceCheckpoint = 0;
private const int CheckpointThreshold = 50;

async Task ProcessEventHandler(ProcessEventArgs args)
{
    try
    {
        // Process event
        string eventBody = Encoding.UTF8.GetString(args.Data.Body.ToArray());

        // Simulate processing
        await ProcessEventAsync(eventBody);

        // Increment counter
        Interlocked.Increment(ref eventsSinceCheckpoint);

        // Checkpoint every 50 events
        if (eventsSinceCheckpoint >= CheckpointThreshold)
        {
            await args.UpdateCheckpointAsync();
            Interlocked.Exchange(ref eventsSinceCheckpoint, 0);
            Console.WriteLine($"Checkpoint saved for partition {args.Partition.PartitionId}");
        }
    }
    catch (Exception ex)
    {
        Console.WriteLine($"Error processing event from partition {args.Partition.PartitionId}: {ex.Message}");
        // Don't checkpoint on error - event will be reprocessed
    }
}

Task ProcessErrorHandler(ProcessErrorEventArgs args)
{
    Console.WriteLine($"Error on partition {args.PartitionId}: {args.Exception.Message}");
    Console.WriteLine($"Operation: {args.Operation}");

    // Implement retry logic or alerting here
    return Task.CompletedTask;
}
```

</details><br/>

> **AZ-204 Exam Tip:**
> - **Event Processor**: Automatic partition distribution and load balancing
> - **Checkpoints**: Track progress per partition in blob storage
> - **Frequency**: Balance between data loss (frequent) and performance (infrequent)
> - **Ownership**: Partitions automatically redistributed if instance fails
> - **Lease Duration**: Default 30 seconds, can be configured
> - **At-least-once**: Events may be redelivered after failure

## AZ-204 Exam Study Points

### Key Concepts to Master

1. **Event Hubs Architecture:**
   - Namespace â†’ Event Hub â†’ Partitions
   - Partitions: Ordered sequences (1-32+)
   - Consumer Groups: Independent views ($Default automatically created)
   - Throughput Units: Capacity units (ingress/egress)

2. **Partitioning:**
   - Partition Key: Same key â†’ same partition (ordering)
   - Partition ID: Direct assignment to partition
   - Round-robin: No key specified
   - Cannot change partition count after creation

3. **Comparison (Event Hubs vs Event Grid vs Service Bus):**
   - **Event Hubs**: Streaming, high throughput, replay
   - **Event Grid**: Reactive, push-based, serverless
   - **Service Bus**: Transactional, FIFO, dead-lettering

4. **SDK Operations:**
   - Producer: SendAsync, CreateBatchAsync
   - Consumer: ReadEventsAsync, ReadEventsFromPartitionAsync
   - Properties: Custom metadata on events
   - Batching: Better performance

5. **Capture:**
   - Automatic capture to Blob/ADLS
   - Avro format
   - Time OR Size window
   - No impact on streaming

6. **Throughput:**
   - Standard: Throughput Units (1-20 TUs)
   - Premium: Processing Units (1-16 PUs)
   - Auto-inflate: Automatic scaling
   - Throttling: ServerBusy errors

7. **Event Processor:**
   - Automatic partition distribution
   - Checkpoint management (blob storage)
   - Load balancing across instances
   - Fault tolerance and recovery

### Common Exam Scenarios

1. **Scenario:** Process IoT telemetry from 100,000 devices
   - **Solution:** Event Hubs with multiple partitions, Event Processor for scale

2. **Scenario:** Ensure events from same device are processed in order
   - **Solution:** Use partition key (device ID) when sending events

3. **Scenario:** Archive streaming data for compliance (7 years)
   - **Solution:** Enable Event Hubs Capture to Blob Storage

4. **Scenario:** Multiple teams need to independently process same events
   - **Solution:** Create separate consumer groups for each team

5. **Scenario:** Handle traffic spikes without manual scaling
   - **Solution:** Enable auto-inflate with appropriate max TUs

6. **Scenario:** Choose between Event Hubs and Service Bus for logs
   - **Solution:** Event Hubs (high throughput, streaming nature)

7. **Scenario:** Recover processing after application crash
   - **Solution:** Event Processor with checkpointing automatically resumes

### Best Practices

1. **Partitioning:**
   - Choose partition key based on ordering requirements
   - More partitions = more parallelism (but more consumers needed)
   - Plan for growth (can't change partition count)

2. **Throughput:**
   - Monitor ingress/egress metrics
   - Enable auto-inflate for variable workloads
   - Consider Premium for consistent high throughput

3. **Processing:**
   - Use Event Processor for production workloads
   - Checkpoint appropriately (balance safety and performance)
   - Handle errors without checkpointing (event redelivery)
   - Scale consumers to match partition count

4. **Capture:**
   - Use for long-term storage and batch analytics
   - Choose appropriate time/size window
   - Consider storage costs vs retention needs

5. **Security:**
   - Use managed identity for authentication
   - Implement network security (VNet, Private Endpoints)
   - Restrict access with SAS policies
   - Enable diagnostic logging

## Cleanup

```bash
az group delete -y -n labs-eventhubs-az204 --no-wait
```

## Additional Resources

- [Azure Event Hubs documentation](https://docs.microsoft.com/en-us/azure/event-hubs/)
- [Event Hubs SDK for .NET](https://docs.microsoft.com/en-us/dotnet/api/overview/azure/messaging.eventhubs-readme)
- [Event Processor SDK](https://docs.microsoft.com/en-us/dotnet/api/overview/azure/messaging.eventhubs.processor-readme)
- [Event Hubs vs Event Grid vs Service Bus](https://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services)
- [Event Hubs Capture](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview)
- [AZ-204 Study Guide](https://learn.microsoft.com/en-us/credentials/certifications/resources/study-guides/az-204)
