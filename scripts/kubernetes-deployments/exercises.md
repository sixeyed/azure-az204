# Kubernetes Deployments - Lab Exercises Narration Script

## Opening

Alright, let's get hands-on with Kubernetes Deployments! In this lab, we'll work through creating, scaling, and updating Deployments. Make sure you have your Kubernetes cluster ready and your terminal open. Let's dive in.

---

## Section 1: Understanding the Deployment YAML

First, let's take a look at what a Deployment actually looks like in YAML format.

Here we have a Deployment manifest. At the top, you'll see the API version "apps/v1" and the kind is "Deployment". These identify what type of resource we're creating.

In the metadata, we're calling this Deployment "whoami". Simple enough, just a descriptive name for our deployment.

Now, the spec section is where it gets interesting. Look at the selector here. It's using matchLabels to find Pods with the label "app: whoami". This is how the Deployment knows which Pods it's responsible for managing. It's a key-value pair that must match exactly.

Below that, we have the template section. This is your Pod blueprint - the specification for what Pods this Deployment should create. Notice in the template metadata, we're applying that same "app: whoami" label. This is crucial! The labels in your Pod template must match the labels in your selector, or Kubernetes will reject the entire manifest with a validation error.

In the template spec, we define our container. We're using a simple "whoami" application image that responds with information about itself - the hostname, IP address, and other metadata. Perfect for testing and demonstrations.

---

## Section 2: Creating Your First Deployment

Let's create this Deployment and see what happens.

We're using kubectl apply with the -f flag, followed by the path to our YAML file.

Great! We get confirmation that the Deployment was created. But remember, the Deployment is a controller - it's not the actual Pod. It's the manager that creates and maintains Pods. Let's see what Pods were created.

We're using kubectl get pods with the -l flag to filter by label. This shows us only Pods with the "app=whoami" label.

There's our Pod! Notice the name? It's "whoami" followed by a random string generated by Kubernetes. The Deployment automatically generates unique names for the Pods it creates. You'll never manually specify Pod names when using Deployments - that's handled automatically to avoid conflicts.

Now let's look at the Deployment itself using kubectl get deployments.

This shows us basic information. We can see one Deployment with one ready replica.

Using kubectl get deployments with the -o wide flag gives us more details, including the container image and selector being used.

Let's get even more information using kubectl describe for the whoami deployment.

Look at all this information! We can see the replica count, the selector labels, the Pod template details, and importantly, the events at the bottom showing what actions Kubernetes took.

Notice it mentions something called a ReplicaSet? We'll come back to that concept later, but for now, just know that Deployments actually use ReplicaSets behind the scenes to manage Pods. It's an abstraction layer.

---

## Section 3: Scaling Deployments

Now, let's say our application is getting more traffic and we need more Pods. We can scale up!

There are two ways to scale a Deployment. First, let's try the imperative way.

We're using kubectl scale with the deploy resource type and whoami name, setting replicas to 3.

We're telling Kubernetes: "Scale the whoami Deployment to three replicas right now."

Let's check the pods with kubectl get pods filtered by our label.

Look at that! We now have three Pods running. Kubernetes quickly created two more Pods from the same template. They all have different generated names.

But here's the problem. Our running Deployment now has three replicas, but our YAML file in source control still says one replica. This is what we call configuration drift, and in production, it's a recipe for disaster.

If someone runs an automated deployment from source control tomorrow, those manual changes will get overwritten and we'll be back to one replica. We'll lose our scale without understanding why.

So, the better approach is to scale declaratively by updating the YAML file and re-applying it.

Here's our updated manifest. It's almost identical to the original, but notice the spec now includes "replicas: 2". This is the declaration of our desired state.

Let's apply this using kubectl apply.

Now check the Pods again with kubectl get pods.

We're back down to two Pods. Kubernetes saw that we had three replicas running, but our desired state says two, so it terminated one Pod. This is the declarative model in action. You declare what you want, and Kubernetes makes it happen by reconciling actual state with desired state.

---

## Section 4: Working with Managed Pods

When you're working with Pods managed by a Deployment, you need to use labels because the Pod names are random and change with each update.

For example, let's look at logs using kubectl logs with a label selector.

Using the label selector, we can see logs from all Pods with that label. This works even when Pod names change.

You can also execute commands at the Deployment level using kubectl exec with deploy/whoami as the target.

Okay, that failed with an error about no shell, but that's expected! This particular container image doesn't have a shell, so we can't execute commands in it. But the syntax is correct for Deployments that do support exec. You reference the deployment, and Kubernetes picks one of the pods.

Let's get detailed information about our Pods using kubectl get pods with -o wide, --show-labels, and a label filter.

Perfect! This shows us the Pod names, their IP addresses, which nodes they're running on, and all their labels. This information is super useful for troubleshooting connection issues or understanding Pod placement.

---

## Section 5: Exposing the Deployment with Services

Now, to actually access our application, we need Services. We have two Service manifests ready: one for a LoadBalancer and one for a NodePort.

Let's apply both at once by pointing to the services directory.

Now let's check the endpoints using kubectl get endpoints for both services.

See those IP addresses? Those are the Pod IPs we saw earlier. The Services use the same label selector to find the Pods, and they're routing traffic to both replicas automatically.

Let's test the application using curl to localhost on port 8080.

There's our response! The application tells us about itself, including hostname and IP information. If we curl again, the Service will load-balance between our two Pods, and you'll see the hostname change.

---

## Section 6: Rolling Updates

Now for one of the coolest features of Deployments: rolling updates. Let's say we want to change our application configuration.

In this updated manifest, we're adding an environment variable that changes how the application responds. Environment variables are fixed for the life of a Pod container, so this change requires new Pods to be created.

Before we apply the change, let's watch what happens to the Pods in real-time.

In one terminal, we'll run kubectl get pods with the watch flag. This will continuously update as things change.

Now, let's apply the update using kubectl apply.

Watch what's happening! Kubernetes is creating new Pods with the updated configuration while the old Pods are still running. Then, once the new Pods are ready and pass their readiness checks, it terminates the old ones. This is a rolling update!

Your application stays available throughout the entire process. No downtime. No dropped requests. That's the power of Deployments managing updates for you.

Let's test the application now using curl.

There we go! The output is now shorter because of our configuration change. If we curl multiple times, we'll see load balancing across both new Pods.

---

## Section 7: Rolling Back Changes

What if we realize this update was a mistake? Maybe users don't like the new format. No problem! Deployments make rollbacks incredibly easy.

First, let's check the rollout history using kubectl rollout history for the whoami deployment.

This shows us the revision history. We can see we're on revision 2 now, and we can see information about each revision.

To roll back to the previous version, it's just one command: kubectl rollout undo for the deployment.

That's it! Kubernetes is now performing another rolling update, but this time it's going back to revision 1. It's creating Pods with the old configuration and terminating the new ones.

Let's check the pods with kubectl get pods.

See the Pod ages? These are fresh Pods running the old configuration. The rolling update brought us back to the previous state.

Let's verify the application with curl.

Perfect! We're back to the full output. We've successfully rolled back our deployment. This is incredibly valuable for production - if a deployment goes wrong, you can quickly revert with a single command.

---

## Section 8: Lab Challenge Introduction

Alright, now it's time for your challenge! Rolling updates are great, but sometimes you need a different deployment strategy.

Your task is to implement a blue-green deployment. That means running two separate Deployments: one for version 1 and one for version 2. Both versions will be running simultaneously, but initially, only version 1 should receive traffic through the Service.

Then, you'll update the Service to switch traffic to version 2 without making any changes to the Deployments themselves. This gives you instant traffic switching and easy rollback capabilities - you just point the service back to version 1.

Here are some hints: Think about how Services find Pods. What selects which Pods receive traffic? Labels! You'll need to use labels creatively here to route traffic between different deployments.

Take some time to work through this on your own. Check the hints file if you get stuck, or look at the solution when you're ready to compare approaches.

---

## Closing

Great work! You've now created Deployments, scaled them declaratively, performed rolling updates, and rolled back changes. These are fundamental skills for managing applications in Kubernetes, and they're essential for the AZ-204 exam.

Remember, Deployments are all about declaring your desired state and letting Kubernetes handle the details. Always use declarative YAML for production environments, and leverage the built-in rollback capabilities when things don't go as planned.

In the next section, we'll focus specifically on how these concepts apply to the AZ-204 certification exam. See you there!
