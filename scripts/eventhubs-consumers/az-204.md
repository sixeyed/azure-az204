# Event Hubs Partitioned Consumers - AZ-204 Exam Preparation

Event Hubs is a key topic in the AZ-204 exam under the develop message-based solutions domain, which accounts for ten to fifteen percent of your score. This lab covers multiple exam objectives related to event processing, consumer patterns, and reliability guarantees. Understanding these concepts isn't just about passing the exam - it's about mastering the patterns that make distributed event processing systems reliable and scalable in production.

## Exam Coverage and Objectives

The exam specifically tests your ability to implement solutions that use Azure Event Hubs. You'll need to demonstrate competency with processing events using Event Hubs, understanding how partition keys and consumer groups work, and managing event retention and capture. These aren't isolated topics - they combine into comprehensive scenarios where you need to choose the right approach for specific requirements.

## Event Hubs Architecture for the Exam

Partitions are fundamental to Event Hubs and appear frequently in exam questions. Understanding that partitions enable parallel processing and scale is essential. The partition count is set at creation and cannot be changed, which is a critical architectural constraint. Events with the same partition key always go to the same partition, enabling ordering guarantees. More partitions cost more but enable greater scale by supporting more concurrent consumers. You can have multiple consumers per partition in different consumer groups.

Consumer groups are another exam favorite. Every Event Hub has a dollar sign Default consumer group that you cannot delete. Multiple consumer groups can read the same events independently, each maintaining its own offset and position. Each consumer group maintains its own checkpoint state separate from others. Consumer groups enable multiple applications to process the same stream simultaneously. The concept is similar to Service Bus topics with multiple subscriptions, but the implementation is different.

Retention capabilities vary by SKU and matter for exam scenarios. Basic SKU provides 1 day retention, Standard SKU extends this to 1-7 days retention, and Premium and Dedicated SKUs support up to 90 days retention. Events are deleted after the retention period expires. Capture can store events beyond retention for long-term analysis, providing archival capabilities that retention alone doesn't offer.

## Event Processing Patterns

The partitioned consumer pattern is critical for exam success. It uses checkpoint-based processing to track progress, storing checkpoints in Azure Blob Storage as JSON blobs. This provides an at-least-once delivery guarantee where every event will be processed at least once, but some might be processed twice in certain failure scenarios. The pattern enables automatic load balancing across consumers without manual coordination. It supports automatic failover when consumers fail, with remaining consumers taking over abandoned partitions.

Understanding at-least-once delivery and its implications is essential. Every event will be processed at least once by the consumer group. Events might be processed multiple times if a consumer crashes after processing but before updating the checkpoint. This is why processing logic should be idempotent when possible, producing the same result whether an event is processed once or multiple times. The checkpoint interval affects the window of potential duplicates - more frequent checkpointing reduces duplicate processing but increases storage operations and costs.

The competing consumer pattern is an architectural pattern you need to recognize. Multiple consumers read from the same event stream, with work distributed across consumers for horizontal scale. Consumers compete for events within their consumer group. This provides high availability through redundancy since if one consumer fails, others continue processing. Event Hubs implements this pattern through partition ownership managed by the SDK.

## SKUs and Features

The exam tests your knowledge of SKU differences and when each is appropriate. Basic SKU provides 1 consumer group per Event Hub, 1 day message retention, and 100 brokered connections, making it good for development and testing but limited for production. Standard SKU offers 20 consumer groups per Event Hub, up to 7 days retention, 1000 brokered connections, includes the Capture feature for archival, supports Kafka protocol, and is required for the partitioned consumer pattern. Premium and Dedicated SKUs provide higher throughput and performance, longer retention up to 90 days, VNet integration for private connectivity, and customer-managed keys for encryption.

## Capture Feature

Event Hubs Capture appears in exam questions about archival and long-term storage. Capture automatically stores events to Blob Storage or Data Lake, using the efficient Avro format that includes schema information. You configure time and size windows determining when new files are created. Capture creates a time-based folder structure organizing events hierarchically by date and time. Enabling Capture does not affect event consumers, who continue reading from the live stream. Capture is available in Standard SKU and higher. It's ideal for compliance archival requirements and batch analytics scenarios where you need to process historical events.

## Connection and Security

Connection strings contain several components you need to understand. The endpoint shows the Event Hubs namespace URL ending in servicebus.windows.net. The Shared Access Key Name identifies the policy providing access. The Shared Access Key provides the authentication credential. The entity path specifying the Event Hub name can optionally be included in the connection string or provided separately.

Authorization uses Shared Access Signatures for connection string-based authentication, which is simple but requires managing secrets. Azure Active Directory authentication is the preferred approach for production security, using managed identities to eliminate secrets from code. You can create namespace-level policies affecting all Event Hubs or entity-level policies for specific Event Hubs. Different permissions include Manage for administrative operations, Send for publishing events, and Listen for consuming events.

Best practices dictate using entity-level policies with least privilege, giving only the permissions needed. Producers should have only Send permission, never Listen or Manage. Consumers should have only Listen permission, never Send or Manage. Avoid using RootManageSharedAccessKey in applications since it provides full access. Rotate keys regularly to limit the impact of compromised credentials.

## Blob Storage Integration

The exam tests how consumers use blob storage for coordination. Checkpoints store the current position or offset in the event stream. They're stored as JSON blobs in the checkpoint container you specify. There's one checkpoint per consumer per partition owned. The checkpoint structure includes the offset showing where processing reached, the sequence number providing an alternative position identifier, and the partition ID identifying which partition this checkpoint covers. Checkpoints enable recovery after consumer failures by letting consumers resume from their last recorded position.

## Configuration and Management

Understanding CLI commands helps with exam questions about resource creation. The command az eventhubs namespace create establishes the namespace container. The command az eventhubs eventhub create establishes the Event Hub itself with partition count and retention. The command az eventhubs eventhub consumer-group create establishes consumer groups. The command az eventhubs namespace authorization-rule keys list retrieves connection strings for authentication.

Key parameters you need to know include partition count which cannot be changed after creation, message retention in days determining how long events are kept, throughput units at the namespace level controlling capacity, SKU selection determining available features, and location with resource group for organizing resources.

## Scaling and Performance

Throughput units define the capacity model for Basic and Standard SKUs. One throughput unit provides 1 MB per second ingress or 2 MB per second egress. Throughput units are set at the namespace level, not per Event Hub. You can scale them up or down, or enable auto-inflate for automatic scaling. Basic and Standard SKUs use this throughput unit model, while Premium uses processing units.

Partition scaling works differently. Each partition can serve one active consumer per consumer group at a time. If you have 5 partitions, you can have a maximum of 5 concurrent consumers per consumer group. Multiple consumer groups can scale independently, each with their own set of consumers. The partition key you use when sending events determines which partition receives each event, enabling you to control event distribution.

## Error Handling and Reliability

Common scenarios you'll encounter in exams include consumers crashing before checkpointing, causing events to be processed twice on recovery. Consumers lagging behind might cause retention periods to be exceeded, losing events. Partition ownership changes trigger automatic rebalancing across available consumers. Network failures are handled by automatic retry logic in the SDK. Throttling requires backing off and retrying after a delay.

Best practices address these challenges. Checkpoint frequently but not after every single event to balance reliability and performance. Monitor consumer lag to detect processing problems before events are lost. Plan partition count for expected scale, remembering you can't change it later. Set appropriate retention periods considering potential lag scenarios. Use multiple consumer instances for high availability, ensuring system can tolerate individual consumer failures.

## Sample Exam Scenarios

Scenario one asks how to ensure multiple instances of a consumer application process different events from an Event Hub. The answer is using multiple consumer instances within the same consumer group using EventProcessorClient. The SDK handles partition distribution automatically.

Scenario two requires processing the same events for both real-time analytics and auditing. The answer is configuring two consumer groups, one for analytics and one for auditing. Each consumer group maintains independent processing position.

Scenario three asks how a consumer application can resume processing after a failure. The answer is checkpointing the offset periodically to blob storage. The checkpoint records where processing reached, enabling resume from that position.

Scenario four involves designing an Event Hub solution expecting to process 10 MB per second of incoming events, asking for the minimum throughput units needed. The answer is 10 throughput units since 1 TU provides 1 MB per second ingress.

Scenario five describes a consumer processing events that, after a crash, processes some events twice, asking about the delivery guarantee. The answer is at-least-once delivery, which is what Event Hubs with checkpointing provides.

## Lab Connection to Exam

This lab demonstrates several exam-critical skills worth reviewing. Infrastructure setup involves creating Event Hub namespace, Event Hub, and consumer groups with appropriate SKU selection. Security work includes working with connection strings and authorization rules following least privilege. Consumer implementation uses EventProcessorClient with checkpoint storage in blob containers. Scaling involves running multiple consumers and observing automatic load balancing. Reliability testing includes failover scenarios and checkpoint recovery. Consumer groups demonstrate using multiple groups for different processing needs. Capture involves configuring automatic event storage to blob storage for archival.

## Additional Study Topics

Beyond this lab, you should also study Event Hubs versus Service Bus, understanding when to use each service based on requirements. Event Grid integration treats Event Hubs as an Event Grid source for routing events. Kafka protocol support allows using Event Hubs with Kafka clients for compatibility. Schema Registry manages event schemas in Premium SKU for schema evolution. Geo-disaster recovery pairs namespaces for regional failover. Monitoring uses metrics and diagnostic logs to track system health. Azure Active Directory integration uses managed identities instead of connection strings for enhanced security.

## Hands-On Practice Recommendations

To prepare for the exam, practice creating Event Hubs with different configurations including various partition counts, retention periods, and SKUs. Write consumer code using EventProcessorClient to understand the programming model. Test checkpoint recovery scenarios by stopping and restarting consumers. Configure multiple consumer groups and verify they process independently. Enable and verify Capture to understand archival. Monitor partition distribution and consumer lag using Portal metrics. Practice with both CLI and Portal since exam questions use both. Implement error handling and retry logic following best practices.

## Key Documentation to Review

Make sure you're familiar with Event Hubs overview documentation covering concepts and architecture. EventProcessorClient API documentation explains programming patterns. Partition keys and routing documentation covers event distribution. Consumer groups deep dive explains isolation and coordination. Checkpointing and error handling patterns show reliability techniques. Scaling guidance explains capacity planning. Security best practices cover authentication and authorization.

The hands-on experience from this lab combined with understanding these concepts prepares you well for Event Hubs questions on the AZ-204 exam. Focus on understanding when to use different features, how patterns like partitioned consumers provide reliability and scale, and how to troubleshoot common scenarios. The exam tests practical application of knowledge to solve real-world problems, not just memorization of facts.
