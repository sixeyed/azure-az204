Excellent work with Event Hubs! This is a key topic for AZ-204's "Develop message-based solutions" domain accounting for 10-15% of the exam.

Partition architecture is fundamental to understanding Event Hubs at scale. Partitions are ordered sequences of events that enable parallelism, and each partition has independent throughput so four partitions give you four times the throughput compared to one partition. The critical exam gotcha is that partition count is set at creation and cannot be changed afterward, just like you saw when you created your Event Hub with five partitions. You must carefully consider partition count upfront because more partitions cost more money but fewer partitions limit your scaling ability. The exam tests understanding of these constraints and how they affect architecture decisions.

Consumer groups enable multiple independent processing pipelines over the same event stream. The default group named $Default is always available in every Event Hub. Additional groups enable scenarios like real-time processing plus batch analytics plus auditing, all reading the same events independently at their own pace, exactly like you created a processing group and an auditing group in the lab. Each consumer group maintains its own checkpoints completely separate from other groups. The exam tests when to use multiple consumer groups versus running multiple consumers within one group, and how consumer groups relate to partition ownership.

Retention policies define how long events are stored in Event Hubs before being automatically deleted. Basic tier provides one day retention. Standard tier offers one to seven days, which is why you were able to set a two-day retention period in the lab. Premium tier supports up to ninety days. The exam tests understanding of retention requirements and choosing appropriate SKUs based on those needs. Remember that captured events stored in blob storage can be kept indefinitely regardless of Event Hub retention settings, which is why the Capture feature you enabled can serve as a long-term audit log.

The partitioned consumer pattern with at-least-once delivery guarantees is heavily tested on the exam. Consumers use checkpoints stored in blob storage to track progress through each partition, automatically rebalance partition ownership when consumers fail or are added, and may process some events multiple times if a consumer crashes before checkpointing but will never lose events. The exam tests understanding of delivery guarantees and the requirement to design idempotent processing that handles duplicate events gracefully, building on what you experienced when you saw consumers not reprocess events after the UpdateCheckpoint call recorded their progress.

SKU differences must be memorized for the exam. Basic tier supports 1-32 partitions, 1 consumer group, 1 day retention, and no capture feature. Standard tier supports 1-32 partitions, 20 consumer groups, 1-7 day retention, and capture to blob storage which you used in the lab. Premium and Dedicated tiers offer higher throughput, longer retention, and network isolation features. The exam tests choosing appropriate SKUs based on specific requirements in scenario questions.

Checkpoint mechanisms use blob storage to record consumer progress with a separate blob per partition per consumer group, exactly what you saw when you explored the Storage Account in the lab. Checkpoints enable fault tolerance and prevent reprocessing all events after failures, but they require idempotent processing because some events may be processed multiple times when a consumer crashes after processing but before checkpointing. The exam tests understanding of checkpoint behavior, what happens during consumer failures like when you stopped a processor midway through, and how checkpoint frequency affects duplicate processing risk.

When to use Event Hubs versus Service Bus is a common scenario question. Choose Event Hubs for high-throughput event streaming, telemetry ingestion, distributed tracing, and clickstream analysis where you need millions of events per second like the device logs scenario in the lab. Choose Service Bus for enterprise messaging with guaranteed order within queues and topics, transactions, and sessions where you need strong messaging semantics. The exam tests identifying which service fits specific requirements.

The exam also covers CLI commands for creating namespaces, event hubs, and consumer groups like you used in the lab, scaling with throughput units for Basic and Standard tiers or processing units for Premium tier, common troubleshooting scenarios around partition balancing and checkpoint failures, and integration with Event Grid for event reactions and Kafka protocol for existing Kafka applications. Master Event Hubs for the AZ-204!
