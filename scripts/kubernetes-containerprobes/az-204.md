# Kubernetes Container Probes - AZ-204 Exam Exercises

Container probes are essential for the AZ-204 exam because they enable self-healing applications and reliable service delivery in Azure Kubernetes Service. The exam tests your understanding of liveness probes that restart unhealthy containers, readiness probes that control traffic routing, startup probes that protect slow-starting applications, and how these mechanisms integrate with Deployments and Services. These exercises focus on practical probe configuration, troubleshooting probe failures, and understanding how probes affect application availability and rolling updates in production AKS clusters.

## Liveness Probes and Container Restart Behavior

Liveness probes determine whether a container is running properly, and understanding their behavior is critical for AZ-204 exam scenarios involving application reliability. When a liveness probe fails, Kubernetes kills the container and restarts it according to the Pod's restart policy. This automatic recovery handles situations where applications enter deadlocked states, become unresponsive due to bugs, or experience resource exhaustion that prevents them from serving requests even though the process is still running.

The exam tests whether you understand that liveness probes should detect application-level failures that restarts can fix, not transient errors that will resolve themselves. A liveness probe that's too aggressive causes unnecessary restarts creating instability. A liveness probe that's too permissive fails to detect real problems allowing unhealthy containers to continue running. Scenarios might present probe configurations and ask whether they're appropriate, testing your ability to balance sensitivity against stability.

Liveness probe configuration involves several parameters that appear in exam questions. The initialDelaySeconds parameter specifies how long to wait after container start before performing the first probe, giving the application time to initialize. The periodSeconds parameter controls how often probes run, with shorter periods detecting failures faster but creating more overhead. The failureThreshold parameter determines how many consecutive failures trigger a restart, with higher thresholds tolerating temporary glitches but delaying detection of real problems.

The exam expects you to understand that liveness probe failures during Deployment rolling updates can cause updates to stall or fail. If new Pods continuously fail liveness checks and restart, the update never completes and you're left with a mix of old and new versions. Knowing how to diagnose this situation using kubectl describe pod to see probe failure events and kubectl rollout status to check update progress demonstrates troubleshooting competence.

## Readiness Probes and Traffic Management

Readiness probes control whether a container receives traffic from Services, and the AZ-204 exam tests this concept through scenarios involving zero-downtime deployments and service availability. When a readiness probe fails, Kubernetes removes the Pod from Service endpoints so it stops receiving requests, but unlike liveness probes, the container is not restarted. This allows containers to signal they're temporarily unavailable - perhaps warming up caches, waiting for database connections, or under temporary overload - without triggering restarts.

The exam tests your understanding that readiness probes enable graceful handling of startup time. Applications often need seconds or minutes to initialize - loading data, establishing connections, warming caches - before they're ready to serve traffic. Readiness probes prevent users from hitting applications before they're ready, avoiding errors and improving perceived reliability. Scenarios involving applications with long startup times or initialization requirements point toward readiness probe configuration.

Readiness probes interact with Deployments during rolling updates in ways the exam tests thoroughly. Kubernetes waits for new Pods to pass readiness checks before considering them available and continuing the update. If new Pods never become ready, the update pauses with some Pods running the old version and some running the new version, all while the old version continues serving traffic. This behavior provides safety during updates but requires correct probe configuration to work properly.

The distinction between liveness and readiness probes is a common exam question. Liveness probes detect when containers need restarting, readiness probes detect when containers shouldn't receive traffic. An application experiencing temporary overload should fail readiness checks to shed load but pass liveness checks to avoid unnecessary restarts. An application in a deadlocked state should fail liveness checks to trigger a restart. Knowing which probe type to use for different failure scenarios demonstrates deep understanding.

## Startup Probes for Slow-Starting Applications

Startup probes protect slow-starting containers from being killed prematurely by liveness probes, and understanding this pattern is important for AZ-204 exam scenarios involving legacy applications or applications with lengthy initialization. Without startup probes, you'd need to set very long initialDelaySeconds on liveness probes to accommodate worst-case startup times, which delays detection of problems after the application is running. Startup probes solve this by disabling liveness and readiness checks until the startup probe succeeds, providing separate configuration for startup versus runtime health.

The exam tests whether you understand when startup probes are necessary. Applications with unpredictable startup times that vary based on data loading, cache population, or external dependencies benefit from startup probes. Applications that need to perform expensive initialization like loading large models or establishing connection pools might require startup probes. Legacy applications migrated to containers that weren't designed for cloud-native environments often need startup probes to handle their startup characteristics.

Startup probe configuration uses the same mechanisms as liveness and readiness probes - HTTP requests, TCP socket checks, or command execution - but with different failure tolerance parameters. The failureThreshold for startup probes is typically much higher than for liveness probes, allowing dozens or hundreds of attempts before giving up. Combined with periodSeconds, this creates a startup window during which the application can take its time initializing without risking termination.

The exam might present scenarios where Pods are stuck in CrashLoopBackOff because they're being killed before finishing startup. This is a classic indicator that startup probes might be needed. Recognizing this pattern and recommending startup probe configuration demonstrates practical troubleshooting skills and understanding of how probe types interact.

## Probe Mechanisms - HTTP, TCP, and Exec

The AZ-204 exam tests your understanding of the three probe mechanisms and when to use each. HTTP GET probes make an HTTP request to a specified path and port, expecting a 200-399 status code for success. This is perfect for web applications and APIs that can expose a health check endpoint. The endpoint should check critical dependencies like database connectivity, not just return a static success response. Scenarios involving web applications or REST APIs typically use HTTP probes.

TCP socket probes attempt to open a TCP connection to a specified port, succeeding if the connection is established. This is useful for services that don't speak HTTP like databases, message queues, or gRPC services. TCP probes verify that the service is listening and accepting connections but can't check application-level health. Scenarios involving non-HTTP services point toward TCP probes.

Command execution probes run a command inside the container and check the exit code, succeeding if the exit code is zero. This provides maximum flexibility because you can run any diagnostic logic your application needs. The command has access to the container filesystem and environment, allowing complex health checks. However, exec probes create more overhead than HTTP or TCP probes and require diagnostic tools to be available in the container. Scenarios requiring custom health check logic that HTTP or TCP probes can't handle point toward exec probes.

The exam might ask you to choose the appropriate probe mechanism given application characteristics. For a REST API returning JSON, HTTP probe checking a dedicated health endpoint is best. For a PostgreSQL database, TCP probe checking port 5432 works well. For a custom batch processor with no network interface, exec probe running a status check command is appropriate. Making these determinations quickly based on application type demonstrates practical knowledge.

## Probe Timing and Failure Thresholds

Understanding probe timing parameters is essential for exam scenarios involving probe tuning and troubleshooting. The initialDelaySeconds parameter gives applications time to start before probing begins, and setting this too low causes false failures during legitimate startup while setting it too high delays detection of problems. Applications with known startup times should set initialDelaySeconds appropriately, while applications with unpredictable startup should use startup probes instead.

The periodSeconds parameter controls probe frequency, with shorter periods detecting failures faster but creating more overhead and potentially overwhelming the application with health check requests. The default ten seconds works for most applications, but high-availability applications needing fast failure detection might use shorter periods while applications sensitive to probe overhead might use longer periods.

The failureThreshold parameter determines how many consecutive probe failures trigger action - restarting for liveness probes, removing from endpoints for readiness probes, or giving up for startup probes. Higher thresholds tolerate transient failures and network glitches but delay detection of real problems. Lower thresholds detect problems quickly but risk false positives from temporary issues.

The successThreshold parameter specifies how many consecutive successes are needed after failures before marking the probe successful again. For liveness and startup probes this is always one - success after restart is immediate. For readiness probes it can be higher to prevent flapping where Pods rapidly enter and leave service. Scenarios involving Pods that alternate between ready and not ready point toward increasing successThreshold on readiness probes.

## Azure Kubernetes Service Integration

The AZ-204 exam emphasizes how container probes integrate with Azure-specific features. Azure Load Balancer health probes are configured automatically based on readiness probes in your Service endpoints, creating a chain where container readiness affects Service endpoints which affects load balancer backend health. Understanding this multi-layer health checking explains how traffic flows in AKS and how probe failures propagate through the system.

Azure Monitor and Container Insights collect probe failure events and can alert on containers failing health checks. Setting up monitoring for probe failures provides visibility into application health and enables proactive response to problems. The exam might ask about implementing health monitoring or troubleshooting using Azure Monitor data, requiring you to understand how Kubernetes probe events surface in Azure monitoring systems.

Azure Application Gateway Ingress Controller uses readiness probes to determine backend health when routing traffic. This integration means probe configuration affects not just Kubernetes Service routing but also Application Gateway traffic distribution. Scenarios involving Application Gateway and AKS require understanding how probes influence traffic at multiple layers.

Azure DevOps and GitHub Actions pipelines should include probe configuration in Deployment manifests, and the exam tests whether you understand that probe configuration is part of infrastructure-as-code practices. Pipelines that deploy applications without proper probe configuration create reliability problems that could have been prevented through proper DevOps practices.

## Troubleshooting Probe Failures

The exam tests practical troubleshooting skills through scenarios involving probe failures and their symptoms. Common issues include Pods stuck in CrashLoopBackOff because liveness probes fail too quickly during startup, Pods that never receive traffic because readiness probes never succeed, rolling updates that stall because new Pods fail readiness checks, and applications experiencing restart loops due to overly aggressive liveness probes.

When troubleshooting probe issues, the diagnostic flow starts with kubectl describe pod to view probe failure events. The events section shows probe failures with timestamps and failure reasons - HTTP status codes for HTTP probes, connection errors for TCP probes, exit codes for exec probes. This information points to whether the problem is application-level health or probe configuration.

Testing probes manually helps isolate issues. For HTTP probes, using kubectl exec to curl the health endpoint from inside the container shows whether the application responds correctly. For TCP probes, using netcat or telnet to check port connectivity verifies the service is listening. For exec probes, running the command manually shows whether it succeeds and what output it produces. These manual tests distinguish application problems from probe configuration problems.

The exam might present scenarios where applications work fine when accessed directly but probe failures prevent them from receiving Service traffic. This points to checking probe configuration against actual application behavior - perhaps the probe path is incorrect, the port doesn't match, or the probe runs before initialization completes. Systematic diagnosis using kubectl commands and manual testing demonstrates the troubleshooting competence that the exam values.

## Cleanup

When you're finished with these exercises, cleaning up removes Pods with their probe configurations. Understanding how probe configuration affects application lifecycle and reliability is important for production environments and exam questions about designing reliable systems. Probes are foundational to Kubernetes self-healing capabilities and proper probe configuration distinguishes production-ready deployments from naive configurations that cause availability problems.
