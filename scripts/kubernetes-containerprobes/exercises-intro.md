We've covered the three types of container probes and why they're essential for production-ready applications. Now let's deploy real applications with different probe configurations and see how Kubernetes responds to failures.

We'll start by looking at the API specs to understand how container probes are structured in the Kubernetes API. Then we'll dive into self-healing apps with readiness probes, where you'll deploy the whoami application and intentionally trigger it to return errors. Without readiness probes, Kubernetes keeps sending traffic to the failed pod, causing user errors, but once you add readiness probe configuration that checks the health endpoint every few seconds, Kubernetes automatically removes failing pods from service endpoints, isolating the problem while keeping the container running.

Next, we'll work with self-repairing apps with liveness probes. These don't just isolate unhealthy pods, they actually restart them. You'll watch as a failed pod first becomes not ready, then after multiple liveness probe failures, the pod restarts and rejoins the service automatically. You'll also explore other probe types beyond HTTP probes, including TCP socket checks that work perfectly for databases and non-HTTP services, simply verifying the port is accepting connections, and exec probes that run commands inside containers for custom health check logic.

The lab challenge presents a Random Number Generator API with reliability problems. You'll need to configure both readiness probes to stop traffic to failed pods and liveness probes to restart them, running multiple replicas with health checks on the healthz endpoint. Finally, we'll do cleanup to remove all the resources we've created. The key learning here is that container probes enable self-healing infrastructure, detecting problems with readiness probes, isolating failed pods from traffic, and automatically recovering with liveness probes, all without manual intervention.
