# Cosmos DB - AZ-204 Exam Exercises

Cosmos DB appears in the AZ-204 exam under the develop for Azure storage domain, which typically accounts for fifteen to twenty percent of your exam score. These exercises go beyond the basics and focus specifically on the advanced features and scenarios you'll encounter in the exam. The AZ-204 exam tests not just your ability to create and use a database, but your understanding of production scenarios like consistency levels, partition strategies, change feeds, and performance optimization.

## Prerequisites

You should complete the basic Cosmos DB lab before diving into these exam-focused exercises. The fundamental concepts like database creation, connection strings, and basic operations are essential building blocks. These advanced exercises assume you're comfortable with the basics and ready to explore the features that distinguish enterprise deployments from simple development scenarios.

## AZ-204 Exam Skills Covered

The exam expects you to demonstrate competency across a wide range of Cosmos DB capabilities. You'll need to perform operations on containers and items using the SDK. Setting appropriate consistency levels for different scenarios is crucial because this represents the fundamental tradeoff between availability, latency, and data consistency. Implementing change feed notifications enables event-driven architectures. Designing effective partition strategies is absolutely critical for Cosmos DB performance and is heavily tested. Configuring indexing policies to optimize performance helps control costs and improve query speed. Finally, optimizing Request Unit consumption is essential for managing expenses.

## Consistency Levels

Consistency levels are a critical exam topic that you must master. Understanding the five consistency levels and when to use each one is fundamental to working with globally distributed databases.

Creating a new account for these exercises allows us to experiment with different settings. When we create the account, we're explicitly setting the consistency level to Session, which is the default and most commonly used level. The enable automatic failover parameter can enhance availability in multi-region deployments.

Strong Consistency provides linearizability guarantees. When you read data, you're absolutely guaranteed to get the most recent committed write. This is the strongest consistency model, but it comes with significant trade-offs. You'll experience the highest latency because reads must coordinate across regions. Reads cost twice as many Request Units compared to other levels. It's really only practical for single-region deployments.

Bounded Staleness guarantees that reads lag behind writes by at most K versions or T time. You configure the acceptable lag, making it predictable. This is excellent for globally distributed applications that need strong consistency with better availability than Strong consistency provides. You can specify the staleness in terms of operations or time, giving you control over the consistency versus performance trade-off.

Session Consistency is the sweet spot for most applications. Within a single session, you get monotonic read and write guarantees, meaning you'll never read old data after reading newer data. Different sessions might see data at different points in time, but within each session, consistency is maintained. This offers an excellent balance of consistency, availability, and latency, which is why it's the default.

Consistent Prefix guarantees that reads never see out-of-order writes. If writes happened in order A, B, C, you might see A, or A and B, but never B without A, or C without B. This is eventual consistency with ordering guarantees, which matters for scenarios where the order of operations is important.

Eventual Consistency is the weakest consistency level with no ordering guarantees. It offers the lowest latency and lowest cost, making it suitable for non-critical data like social media likes, view counters, or other scenarios where absolute consistency isn't required.

You can change the default consistency level on your account at any time. When updating to Bounded Staleness, you specify the max staleness prefix and max interval parameters. An important exam point to remember is that you can always relax consistency on individual requests, but you cannot strengthen it beyond the account's default level.

## Partition Keys and Container Design

Partition key selection is absolutely critical for Cosmos DB performance and is heavily tested on the exam. Choosing the wrong partition key can result in hot partitions, poor performance, and excessive costs.

When creating a database, you're establishing the logical container that will hold your collections. Then when creating a container, the partition key path uses JSON notation. In our examples, we're partitioning on the category property, which means all items with the same category value will be stored together on the same logical partition.

For the exam, you need to understand what makes a good partition key. High Cardinality means the partition key should have many distinct values, allowing Cosmos DB to distribute data across many partitions. Even Distribution ensures data is evenly distributed across partition key values. You want to avoid hot partitions where one partition key gets significantly more traffic than others. Query Patterns matter because your most common queries should include the partition key in the WHERE clause. This allows Cosmos DB to route queries to specific partitions rather than scanning all partitions. The most critical point is that you cannot change the partition key after container creation, so choose carefully.

Good partition key examples that might appear in exam scenarios include using userId for user data, where each user is a partition. This is perfect for applications where you primarily query data for a specific user. For multi-tenant applications, using tenantId means each tenant's data is isolated in its own partition, providing natural data segregation and efficient queries. For product catalogs, using category can work, but only if categories are well-balanced. If ninety percent of products are in one category, this would be a poor choice. For IoT telemetry, using deviceId means each device's data is in its own partition, enabling efficient queries for a specific device's history.

Bad partition key examples that the exam loves to test include status fields. If you use status with only values like active and inactive, you'll have only two partitions, creating hot spots. Date fields like createdDate create hot partitions because all new writes go to the current date's partition. Unbalanced categories, like using region when ninety percent of your data is in one region, creates an extremely hot partition that will throttle performance.

## SDK Operations

The exam tests your knowledge of performing CRUD operations using the Cosmos DB SDK. Understanding the .NET SDK patterns is essential.

Initializing the client requires the account endpoint and authentication key. In production, you'd retrieve these from Azure Key Vault or App Configuration rather than hardcoding them.

Create operations require providing both the item and the partition key. The partition key must be provided when creating items, and you should always check the RequestCharge property in the response to monitor RU consumption. The id field must be unique within the partition.

Read operations are most efficient when you perform point reads. ReadItemAsync with both id and partition key is the most efficient operation, costing only one RU for a one KB item. This is dramatically cheaper than any query operation.

Update operations can use ReplaceItemAsync to replace the entire item, or PatchItemAsync for partial updates, which can be more efficient when you're only changing a few properties.

Delete operations require both the id and partition key, just like read operations. You should monitor the RU charge for deletes as well.

Query operations have dramatically different costs depending on whether you include the partition key. An efficient query with partition key specified allows Cosmos DB to route the query to one specific partition. Using parameterized queries with WithParameter prevents SQL injection and allows query plan caching. Specifying the PartitionKey in QueryRequestOptions ensures the query only runs against one partition.

Cross-partition queries that don't specify a partition key run across all partitions. This is much more expensive in terms of RUs and latency. The exam tip here is to always include the partition key in queries when possible. Cross-partition queries should be avoided in production for frequently-run queries.

## Change Feed

The change feed is a critical topic for the exam, especially for event-driven architecture questions. Understanding how it works and when to use it is essential.

Change feed provides a persistent, ordered log of changes to items in a container. Every insert and update is captured and can be processed by one or more consumers. This enables reactive architectures where changes automatically trigger downstream processing.

The change feed processor pattern involves monitoring a container for changes and processing them through a delegate function. You need a lease container that tracks which changes have been processed, enabling multiple consumers and resumption after failures. The processor name identifies this particular change feed consumer, and the instance name identifies this specific running instance of the processor.

For the exam, remember these critical points. Changes are ordered per partition key, but not across partition keys. The change feed captures inserts and updates only, NOT deletes. If you need to track deletions, implement soft deletes with a deleted flag. The delivery guarantee is at-least-once, meaning changes might be delivered multiple times, so your processing logic should be idempotent. Multiple consumers are supported through the lease container mechanism. Changes remain in the change feed indefinitely, unlike message queues that expire messages.

Change feed use cases that appear in exam scenarios include real-time analytics, where you stream changes to an analytics pipeline for real-time insights. Event-driven architecture scenarios trigger Azure Functions when data changes, enabling reactive workflows. Data replication syncs data to other databases or services for different access patterns. Materialized views maintain denormalized views optimized for specific queries. Audit trail scenarios track all changes for compliance and auditing requirements.

## Indexing Policies

Indexing policies are crucial for optimizing both performance and cost. Understanding the trade-offs is essential for the exam.

By default, Cosmos DB automatically indexes all properties in all items. This is convenient but might not be optimal for your workload. You can check the current indexing policy using the CLI to see what's being indexed.

Custom indexing policies let you specify exactly which paths to index and which to exclude. The indexing mode consistent means indexes are updated synchronously with writes. The automatic flag enables automatic indexing. Included paths specify which properties to index and what type of index to use. Range indexes support both equality and range queries. Excluded paths specify large properties that won't be queried, like descriptions or images.

The trade-offs are clear. More indexes mean faster reads and queries, but slower writes because indexes must be updated, and higher storage costs. Fewer indexes mean slower reads especially for non-indexed properties, but faster writes and lower storage costs. The exam tip is to exclude large properties like images, long text descriptions, or binary data that won't be queried.

## Request Units and Throughput

Understanding Request Units is essential for the exam, as many questions involve optimizing costs and performance.

Database-level throughput is shared across all containers. This allocates RU per second shared among all containers in the database. Container-level throughput is dedicated to one container. This container gets dedicated RU per second that isn't shared with other containers. Autoscale automatically adjusts based on load, scaling between ten percent of max and the specified maximum based on actual usage.

Monitoring RU consumption should happen throughout development. Always check the RequestCharge property on responses to understand the cost of your operations.

RU optimization strategies include several key approaches. Include partition key in queries - this is the single most important optimization because queries with a partition key only scan one partition. Use point reads with ReadItemAsync using both id and partition key, which costs only one RU for a one KB item. Limit query results using MaxItemCount to paginate and avoid scanning entire containers. Index only needed properties to reduce write costs by excluding large or rarely-queried properties from indexing. Use appropriate consistency because Strong consistency costs twice as much as Session consistency for reads. Batch operations process multiple items together for better efficiency. Cache frequently accessed data using Azure Cache for Redis to reduce Cosmos DB queries for hot data.

The exam tip for cost optimization questions is that they almost always involve checking if the partition key is included in queries.

## Time to Live

TTL automatically expires items after a specified period, reducing storage costs and simplifying data management.

Enabling TTL on a container sets a default TTL in seconds for all items. You can also set TTL on individual items to override the container default.

For the exam, remember these TTL values. Negative one means the item never expires, even if the container has a default TTL. Null means the item inherits the container's default TTL. A positive number means the item expires after N seconds from the last modified timestamp.

Use cases include session data, temporary caches, event logs, and any data with a natural expiration.

## AZ-204 Exam Study Points

Let's consolidate the key points you need to master. Cosmos DB supports five APIs: NoSQL which is native and previously called SQL, MongoDB for MongoDB compatibility, Cassandra for Cassandra compatibility, Gremlin for graph databases, and Table for Azure Table Storage compatibility.

The five consistency levels from strongest to weakest are Strong, Bounded Staleness, Session which is the default, Consistent Prefix, and Eventual.

Partition strategy key points include that logical partitions can be up to twenty GB each, physical partitions are managed by Cosmos DB, you should choose high-cardinality partition keys, you cannot change the partition key after creation, and you should always include partition key in queries when possible.

The three throughput models are Provisioned with fixed RU per second and predictable cost best for steady workloads, Autoscale with dynamic scaling and pay for actual usage best for variable workloads, and Serverless with pay per request best for sporadic workloads.

Change feed characteristics to remember are that it's ordered per partition key not across partitions, it captures inserts and updates only with no deletes, it provides at-least-once delivery guarantee, multiple consumers are supported, and changes persist indefinitely.

Query optimization critical points are to include partition key in WHERE clause, use point reads with ReadItemAsync when you know the id and partition key, avoid SELECT star and specify only needed properties, use appropriate indexes, and monitor RU charges during development.

## Common Exam Scenarios

Typical exam scenarios test your ability to apply knowledge. If a globally distributed app needs strong consistency, you use Strong consistency level, but understand this limits write availability in multi-region scenarios because Strong consistency requires quorum coordination across regions.

To process every change to items in real-time for analytics, you implement a Change Feed Processor that monitors the container and streams changes to your analytics pipeline.

To optimize query performance for specific properties that are frequently queried, you create a custom indexing policy with included paths for those properties and excluded paths for large or rarely-queried properties.

To automatically delete old data to reduce storage costs, you set TTL at the container level for a default expiration, or on individual items for item-specific expiration.

To reduce RU consumption for queries that are running expensive cross-partition scans, you redesign queries to include the partition key, or consider changing the partition key strategy if queries consistently need to scan all partitions.

## Best Practices Summary

For the exam and real-world applications, follow these guidelines. Choose partition keys carefully by considering query patterns, data distribution, and cardinality. Use Session consistency because it's the right choice for most applications. Customize indexing policies by excluding unnecessary paths to optimize write performance and costs. Always include partition key in queries to avoid cross-partition queries when possible. Monitor RU consumption by tracking RU charges and identifying optimization opportunities. Use the latest SDK to take advantage of bulk operations and other performance features. Test with production-like data because performance characteristics change significantly with scale.

## Cleanup

When you're finished with these exercises, removing the resource group deletes everything including all databases, containers, data, and associated resources. The no-wait flag lets the deletion proceed in the background so you can continue working. This cleanup is important not just for cost management but also because the exam sometimes includes questions about proper resource lifecycle management and cost optimization.
