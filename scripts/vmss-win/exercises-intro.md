We've covered what Virtual Machine Scale Sets are and why managing multiple instances as a single resource simplifies operations at scale. Now let's build a Windows VMSS from a custom image and explore scaling capabilities.

In the Portal, you explore the VMSS creation experience by searching for and selecting Virtual Machine Scale Set. You see the usual VM settings like image, size, and disks, plus the choice of Orchestration mode where uniform is the most common for identical instances. Under the Scaling section, you select the number of VM instances in the scale set. When you change the scaling policy to Autoscaling, you see scale-out and scale-in thresholds with instance counts, allowing Azure to automatically adjust capacity based on metrics.

Creating a VMSS from a Custom Image uses the image you created and moved or copied to the labs-vmss-win resource group. You use az vmss create specifying the VM SKU, instance count of 3, backend port 3389 for RDP access, the image name, and admin credentials. When the VMSS is created, you check to see the VM list and discover there are no individual VMs because you manage instances through the VMSS instead. Opening the VMSS in the portal shows three instances in the Instances blade, possibly in different statuses like Running or Updating with possibly non-sequential numbers. The instances have private IP addresses but no public ones. The public IP address in the resource group is associated to a load balancer. Browsing to the public IP address doesn't show the app because the load balancer has no rules yet.

Load Balancer Configuration reveals why the app doesn't work initially. Creating the VMSS sets up the load balancer as a networking component listening on the Public IP, but the VMSS setup doesn't include any load balancing rules by default so the LB has no routing table. You open the LB in the portal and confirm Backend pools contain the VMSS instances all running. Health probes show none configured yet, which is how the LB checks that resources are ready to receive traffic. Load balancing rules show none, which is why the app doesn't work. You add a rule to listen on the frontend PIP and route to the VMSS backend pool using port 80, and you add a health probe using HTTP type. Load balancers only send traffic to healthy endpoints. When you browse to the PIP again, the app works, and refreshing shows different VM names in responses as the load balancer shares requests between the three VMs.

Scaling VMSS demonstrates both manual and automatic approaches. You use curl commands to make repeated requests seeing different VM names in HTML responses. You scale up to five instances using az vmss scale with new-capacity parameter. Checking in the portal shows new instances listed in the VMSS blade, automatically added to the LB backend pool, and becoming valid targets when healthy. Windows VMs take a few minutes to commission. You may see more than 5 instances temporarily because Azure overprovisions, creating extra instances to ensure the desired count reaches healthy status quickly, then removing the extras. You change VMSS setup in the Portal to use autoscaling with minimum 2 VMs and maximum 3, with scale-out if CPU greater than 10 percent and scale-in if CPU less than 8 percent using short 2-minute timescales. Your VMSS with 5 instances before switching to autoscale brings down to maximum of 3 after a few minutes, then to minimum of 2 after more minutes with no activity.

VMSS provides centralized management with automatic load balancing and flexible scaling, both manual control and metric-based autoscaling to handle varying workloads efficiently.
