# Azure Blob Storage - AZ-204 Exam Focus
## Exam Preparation Narration Script

*Duration: Comprehensive exam preparation*

---

## Introduction to AZ-204 Blob Storage Topics

Welcome to the AZ-204 exam-focused session on Azure Blob Storage. This content maps directly to the "Develop for Azure Storage" domain, which accounts for 15 to 20 percent of your exam score.

We'll cover the specific skills Microsoft tests on the AZ-204 exam, including metadata management, lifecycle policies, blob versioning, static website hosting, and advanced security features. Let's dive in!

---

## Exam Domain Coverage

The AZ-204 exam tests your ability to:
- Set and retrieve properties and metadata
- Perform operations on data using the appropriate SDK
- Implement storage policies and data lifecycle management
- Implement static website hosting

We'll address each of these areas with hands-on examples and exam tips throughout this session.

---

## Exercise 1: Working with Blob Metadata and Properties

### Understanding Metadata vs Properties

Before we start, let's clarify a critical distinction that appears frequently on the exam:

**Properties** are system-defined attributes like Content-Type, Content-Length, ETag, and Last-Modified. You can view and sometimes modify these, but you can't create new properties.

**Metadata** consists of user-defined key-value pairs that you attach to blobs. This is your custom data about the blob, not the blob content itself.

### Setting Metadata at Upload Time

When uploading a blob, you can attach metadata immediately. This is a common exam scenario - how do you categorize or tag blobs for your application without modifying the content?

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage blob upload \
  --account-name <sa-name> \
  --container-name labs \
  --name test-file.txt \
  --file README.md \
  --metadata department=engineering owner=alice project=az204
```

Notice we're providing multiple metadata key-value pairs. The keys are "department," "owner," and "project," with their respective values.

### Retrieving Metadata

To retrieve just the metadata without downloading the blob content:

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage blob metadata show \
  --account-name <sa-name> \
  --container-name labs \
  --name test-file.txt
```

*[SHOW ON SCREEN: JSON output showing metadata]*

### Updating Metadata

Metadata can be updated independently of the blob content. This is useful for updating tags, status flags, or other tracking information.

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage blob metadata update \
  --account-name <sa-name> \
  --container-name labs \
  --name test-file.txt \
  --metadata department=sales owner=bob status=approved
```

**Exam Tip**: Metadata is often used for tagging, categorization, and custom application logic without modifying the blob content itself. Expect exam questions about when to use metadata versus when to use blob properties.

### Working with Blob Properties

Now let's look at system properties. We can query specific properties using the `--query` parameter:

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage blob show \
  --account-name <sa-name> \
  --container-name labs \
  --name test-file.txt \
  --query '{contentType:properties.contentType, contentLength:properties.contentLength, blobType:properties.blobType, accessTier:properties.accessTier, lastModified:properties.lastModified, etag:properties.etag}'
```

*[SHOW ON SCREEN: JSON output showing properties]*

One property you can modify is the content type. This affects how browsers handle the file when it's downloaded:

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage blob update \
  --account-name <sa-name> \
  --container-name labs \
  --name test-file.txt \
  --content-type text/markdown
```

**Exam Tip**: Know which properties can be modified and which are read-only. Content-Type, Content-Encoding, and Cache-Control are commonly modified properties.

---

## Exercise 2: Blob Lifecycle Management Policies

### Why Lifecycle Management Matters

Lifecycle management is a critical exam topic. In production scenarios, you don't want to manually move files between tiers or delete old data. Lifecycle policies automate this based on rules you define.

**This is a favorite exam topic** - you'll likely see questions about creating policies for specific scenarios.

### Creating a Basic Lifecycle Policy

Lifecycle policies are defined in JSON format. Let's create a policy that demonstrates the typical blob lifecycle:

*[SHOW ON SCREEN: JSON file being created]*

```json
{
  "rules": [
    {
      "enabled": true,
      "name": "move-to-cool-after-30-days",
      "type": "Lifecycle",
      "definition": {
        "actions": {
          "baseBlob": {
            "tierToCool": {
              "daysAfterModificationGreaterThan": 30
            },
            "tierToArchive": {
              "daysAfterModificationGreaterThan": 90
            },
            "delete": {
              "daysAfterModificationGreaterThan": 365
            }
          }
        },
        "filters": {
          "blobTypes": [
            "blockBlob"
          ]
        }
      }
    }
  ]
}
```

Let me explain this policy:
- After 30 days without modification, blobs move to Cool tier
- After 90 days, they move to Archive tier
- After 365 days, they're deleted
- This only applies to block blobs

### Applying the Lifecycle Policy

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage account management-policy create \
  --account-name <sa-name> \
  --policy @lifecycle-policy.json
```

### Verifying the Policy

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage account management-policy show \
  --account-name <sa-name>
```

### Advanced Filtering with Prefix Match

A common exam scenario: different lifecycle rules for different types of data. For example, log files might need faster archival than user documents.

Let's create a policy specifically for logs:

*[SHOW ON SCREEN: JSON file being created]*

```json
{
  "rules": [
    {
      "enabled": true,
      "name": "logs-archival",
      "type": "Lifecycle",
      "definition": {
        "actions": {
          "baseBlob": {
            "tierToCool": {
              "daysAfterModificationGreaterThan": 7
            },
            "tierToArchive": {
              "daysAfterModificationGreaterThan": 30
            },
            "delete": {
              "daysAfterModificationGreaterThan": 180
            }
          }
        },
        "filters": {
          "blobTypes": ["blockBlob"],
          "prefixMatch": ["logs/"]
        }
      }
    }
  ]
}
```

Notice the `prefixMatch` filter - this policy only applies to blobs whose names start with "logs/". This is how you create different policies for different data categories.

### Critical Exam Knowledge: Access Tier Differences

**You must understand these differences for the exam:**

**Hot Tier:**
- Optimized for frequently accessed data
- Highest storage cost
- Lowest access cost
- No minimum storage duration
- Immediate availability

**Cool Tier:**
- For infrequently accessed data
- Lower storage cost than Hot
- Higher access cost than Hot
- Minimum 30-day storage duration
- Immediate availability
- Early deletion fees apply

**Archive Tier:**
- For rarely accessed data
- Lowest storage cost
- Highest access cost
- Minimum 180-day storage duration
- **Offline storage** - not immediately accessible
- Rehydration can take hours
- Significant early deletion fees

**Exam Tip**: Expect questions about which tier to use for different scenarios. Remember: Hot for active data, Cool for 30+ day storage, Archive for 180+ day long-term retention.

---

## Exercise 3: Advanced SAS Token Management

### Understanding SAS Token Types

The exam tests three types of SAS tokens:

1. **Account-level SAS**: Access to multiple services (Blob, File, Queue, Table)
2. **Service-level SAS**: Specific to one service
3. **User delegation SAS**: Secured with Azure AD credentials - **This is the most secure option**

### Creating Container-Level SAS Tokens

Let's create a container-level SAS token with specific permissions:

*[SHOW ON SCREEN: Command being typed]*

```bash
STORAGE_KEY=$(az storage account keys list \
  --account-name <sa-name> \
  --query '[0].value' -o tsv)
```

Note: we're retrieving the storage account key first. In production, you'd want to use Azure AD authentication instead, but understanding key-based access is important for the exam.

Now let's generate a SAS token with read and list permissions:

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage container generate-sas \
  --account-name <sa-name> \
  --account-key <storage-account-key> \
  --name labs \
  --permissions rl \
  --expiry $(date -u -d "2 hours" '+%Y-%m-%dT%H:%MZ') \
  --https-only
```

### SAS Permission Codes

**Memorize these for the exam:**
- **r**: Read - download blob content and properties
- **w**: Write - upload new blobs
- **d**: Delete - delete blobs
- **l**: List - list blobs in container
- **a**: Add - append to append blobs
- **c**: Create - create new blobs

You can combine permissions: "rl" means read and list, "wdl" means write, delete, and list.

### Creating Write-Only SAS Tokens

Here's an exam scenario: a user needs to upload files but should not be able to read existing files.

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage container generate-sas \
  --account-name <sa-name> \
  --account-key <storage-account-key> \
  --name labs \
  --permissions wdl \
  --expiry $(date -u -d "24 hours" '+%Y-%m-%dT%H:%MZ') \
  --https-only
```

Notice: write, delete, and list permissions, but **not** read. This is a common security pattern for upload scenarios.

### IP-Based SAS Restrictions

You can restrict SAS tokens to specific IP addresses or ranges. This is important for security-sensitive scenarios:

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage container generate-sas \
  --account-name <sa-name> \
  --account-key <storage-account-key> \
  --name labs \
  --permissions r \
  --expiry $(date -u -d "1 hour" '+%Y-%m-%dT%H:%MZ') \
  --ip "203.0.113.0-203.0.113.255" \
  --https-only
```

**Exam Tip**: SAS tokens can be restricted by:
- IP address or IP range
- Start and expiry time
- Protocol (HTTPS only recommended)
- Associated with stored access policies for revocation capability

Remember: Only stored access policy-based SAS tokens can be revoked before expiry!

---

## Exercise 4: Blob Leasing for Concurrency Control

### Understanding Blob Leases

Blob leasing is an advanced topic that provides pessimistic concurrency control. This prevents multiple clients from modifying or deleting a blob simultaneously.

**Exam scenario**: Your application needs to ensure only one instance can modify a configuration blob at a time.

### Acquiring a Lease

*[SHOW ON SCREEN: Command being typed]*

```bash
LEASE_ID=$(az storage blob lease acquire \
  --account-name <sa-name> \
  --container-name labs \
  --blob-name test-file.txt \
  --lease-duration 60 \
  --query leaseId -o tsv)

echo "Lease ID: $LEASE_ID"
```

We've acquired a 60-second lease on the blob. During this time, only operations that include this lease ID can modify or delete the blob.

### Testing Lease Protection

Let's try to delete the blob without providing the lease ID:

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage blob delete \
  --account-name <sa-name> \
  --container-name labs \
  --name test-file.txt
```

*[SHOW ON SCREEN: Error message about active lease]*

The operation fails because there's an active lease!

Now let's delete it with the lease ID:

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage blob delete \
  --account-name <sa-name> \
  --container-name labs \
  --name test-file.txt \
  --lease-id <lease-id>
```

*[SHOW ON SCREEN: Successful deletion]*

**Exam Tip**:
- Lease duration: 15-60 seconds or infinite
- Use cases: Distributed locking, ensuring exclusive access
- All write operations require the lease ID when a lease is active
- Leases can be renewed, changed, released, or broken

---

## Exercise 5: Static Website Hosting

### Why Static Website Hosting Matters

Static website hosting is a popular exam topic. You can host entire static websites directly from Blob Storage without needing App Service or a VM.

### Enabling Static Website Hosting

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage blob service-properties update \
  --account-name <sa-name> \
  --static-website \
  --index-document index.html \
  --404-document 404.html
```

This command:
- Enables static website hosting
- Sets index.html as the default document
- Sets 404.html as the error page
- Automatically creates a `$web` container

### Creating and Uploading Content

Let's create a simple HTML page:

*[SHOW ON SCREEN: HTML file being created]*

```html
<!DOCTYPE html>
<html>
<head>
    <title>AZ-204 Static Site</title>
</head>
<body>
    <h1>Azure Blob Storage Static Website</h1>
    <p>This site is hosted on Azure Blob Storage for AZ-204 exam prep!</p>
</body>
</html>
```

Now upload it to the special `$web` container:

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage blob upload \
  --account-name <sa-name> \
  --container-name '$web' \
  --name index.html \
  --file index.html \
  --content-type text/html
```

### Getting the Website URL

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage account show \
  --name <sa-name> \
  --query 'primaryEndpoints.web' -o tsv
```

*[SHOW ON SCREEN: URL displayed]*

The URL follows this pattern: `https://<sa-name>.z6.web.core.windows.net/`

### Creating a Custom 404 Page

*[SHOW ON SCREEN: HTML file being created]*

```html
<!DOCTYPE html>
<html>
<head>
    <title>404 - Page Not Found</title>
</head>
<body>
    <h1>404 - Page Not Found</h1>
    <p>The page you are looking for doesn't exist.</p>
</body>
</html>
```

Upload it:

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage blob upload \
  --account-name <sa-name> \
  --container-name '$web' \
  --name 404.html \
  --file 404.html \
  --content-type text/html
```

**Exam Tip**:
- Static websites are publicly accessible by default
- Use Azure CDN for custom domains and HTTPS
- The `$web` container is created automatically
- Great for SPAs (Single Page Applications)
- Cost-effective alternative to App Service for static content

---

## Exercise 6: Blob Versioning and Soft Delete

### Understanding Data Protection Features

The exam tests your knowledge of protecting data against accidental deletion or modification. Azure Blob Storage offers two key features:

1. **Soft Delete**: Retains deleted blobs for a recovery period
2. **Versioning**: Maintains previous versions of blobs automatically

### Enabling Blob Versioning

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage account blob-service-properties update \
  --account-name <sa-name> \
  --enable-versioning true
```

With versioning enabled, every modification creates a new version. The current version is what users see, but previous versions are preserved.

### Enabling Soft Delete

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage account blob-service-properties update \
  --account-name <sa-name> \
  --enable-delete-retention true \
  --delete-retention-days 7
```

This retains deleted blobs for 7 days before permanent deletion.

### Testing Versioning

Let's create multiple versions of a blob:

*[SHOW ON SCREEN: Commands being typed]*

```bash
echo "Version 1" > test.txt
az storage blob upload --account-name <sa-name> --container-name labs --name test.txt --file test.txt --overwrite

echo "Version 2" > test.txt
az storage blob upload --account-name <sa-name> --container-name labs --name test.txt --file test.txt --overwrite
```

Now list all versions:

*[SHOW ON SCREEN: Command being typed]*

```bash
az storage blob list \
  --account-name <sa-name> \
  --container-name labs \
  --prefix test.txt \
  --include v \
  --query '[].{name:name, version:versionId, isCurrentVersion:isCurrentVersion}' \
  -o table
```

*[SHOW ON SCREEN: Table showing multiple versions]*

**Exam Tip**:
- Versioning is automatic - no manual version management needed
- Soft delete protects against accidental deletion
- Both features can be enabled simultaneously for maximum protection
- Point-in-time restore is available for disaster recovery scenarios
- Immutable storage with legal hold and time-based retention policies provides compliance features

---

## AZ-204 Exam Study Points - Critical Concepts

### 1. Blob Types - Know the Differences

**Block Blobs:**
- Most common type
- For text and binary data
- Documents, images, videos
- Optimized for streaming
- Uploaded in blocks (max 4.75 TB)

**Append Blobs:**
- Optimized for append operations
- Perfect for log files
- Cannot modify existing data, only append
- Max size 195 GB

**Page Blobs:**
- For random read/write operations
- Used for VHD files (virtual machine disks)
- Max size 8 TB
- Optimized for frequent random access

**Exam question pattern**: "Your application needs to store [description of use case]. Which blob type should you use?"

### 2. Access Tiers - Cost vs Performance

Review the comparison:
- **Hot**: Frequently accessed, high storage cost, low access cost, no minimum
- **Cool**: 30+ days retention, lower storage cost, higher access cost, 30-day minimum
- **Archive**: 180+ days retention, lowest storage cost, highest rehydration cost, 180-day minimum, offline

**Exam question pattern**: "Your application has [description of access pattern]. Which tier provides the best balance of cost and performance?"

### 3. SAS Token Types - Security Models

**Account-level SAS:**
- Access across multiple services
- More permissions, less granular
- Used for admin-level operations

**Service-level SAS:**
- Single service (Blob, File, Queue, or Table)
- More granular control
- Common for application access

**User delegation SAS:**
- **Most secure option**
- Uses Azure AD credentials
- Recommended for production
- Cannot be created with account keys

**Exam tip**: Questions about "most secure" authentication method → User delegation SAS

### 4. Lifecycle Management - Automation Patterns

**Rule components you need to know:**
- **Actions**: tierToCool, tierToArchive, delete
- **Filters**: blobTypes, prefixMatch
- **Conditions**: daysAfterModificationGreaterThan, daysAfterLastAccessTimeGreaterThan

**Common exam scenario**: "Automatically archive logs older than 90 days and delete after 365 days"

**Solution**: Create lifecycle policy with tierToArchive at 90 days and delete at 365 days, filtered by prefix "logs/"

### 5. Metadata vs Properties - Critical Distinction

**Properties (System-defined):**
- Content-Type
- Content-Length
- ETag
- Last-Modified
- Content-Encoding
- Cache-Control
- Some can be modified, some are read-only

**Metadata (User-defined):**
- Custom key-value pairs
- Up to 8 KB total
- Used for categorization, tagging, custom logic
- Independent of blob content

**Exam question pattern**: "Your application needs to store [custom information] about each blob. How should you implement this?"

---

## Common Exam Scenarios - Question Patterns

### Scenario 1: Temporary Access to Private Blob

**Question**: "An application needs to provide temporary read access to a private blob for external users. The access should expire automatically and should be revocable if needed. What should you implement?"

**Answer**: Generate a Service-level SAS token backed by a stored access policy. The policy allows revocation even before expiry.

**Key points**: Stored access policy enables revocation, SAS provides time-limited access

### Scenario 2: Automatic Data Archival

**Question**: "Your application generates log files that must be retained for compliance. Logs are actively accessed for 7 days, occasionally accessed for 30 days, and rarely accessed after that. After 2 years, logs must be deleted. Minimize storage costs."

**Answer**: Implement a lifecycle management policy:
- Keep in Hot for 7 days
- Move to Cool at 7 days
- Move to Archive at 30 days
- Delete after 730 days (2 years)

**Key points**: Use lifecycle policies for automatic tier transitions

### Scenario 3: Static Website with Custom Domain

**Question**: "You need to host a static website with a custom domain and HTTPS. What Azure services should you use?"

**Answer**:
1. Enable static website hosting on a storage account
2. Upload content to the $web container
3. Configure Azure CDN with a custom domain
4. Enable HTTPS on the CDN endpoint

**Key points**: Storage alone doesn't support custom domains, need CDN for that

### Scenario 4: Prevent Accidental Deletion

**Question**: "Your application stores critical documents in Blob Storage. You must protect against accidental deletion while allowing users to update documents. What should you implement?"

**Answer**: Enable both soft delete with retention period (e.g., 7-30 days) and blob versioning. Consider immutable storage for regulatory compliance.

**Key points**: Soft delete + versioning provides comprehensive protection

### Scenario 5: Concurrency Control

**Question**: "Multiple instances of your application might attempt to modify the same configuration blob simultaneously. How do you ensure only one instance can modify it at a time?"

**Answer**: Use blob leasing to acquire an exclusive lock. Only the instance holding the lease can modify the blob.

**Key points**: Leases provide pessimistic concurrency control

---

## Performance and Optimization Tips for the Exam

### Understand These Patterns:

1. **Parallel Uploads**: Use block upload for large files, upload blocks in parallel
2. **Content Delivery**: Use Azure CDN for globally distributed read-heavy workloads
3. **Access Patterns**: Choose tier based on actual access frequency, not just age
4. **Metadata Queries**: Blob metadata can be queried without downloading content
5. **Batch Operations**: Use batch operations (like upload-batch) for multiple files

---

## Security Best Practices - Exam Focus

**Know these security patterns:**

1. **Least Privilege**: Grant minimum necessary permissions via SAS
2. **Azure AD Integration**: Prefer user delegation SAS over key-based SAS
3. **HTTPS Only**: Always require HTTPS for SAS tokens
4. **IP Restrictions**: Limit SAS token usage to specific IP ranges when possible
5. **Short Expiry**: Use shortest practical expiry times for SAS tokens
6. **Stored Access Policies**: Enable revocation by using policies
7. **Network Rules**: Configure firewall and virtual network access
8. **Encryption**: Data encrypted at rest by default, use customer-managed keys for additional control

---

## Final Exam Preparation Checklist

Before taking the AZ-204 exam, ensure you can:

✓ Explain the difference between blob types and when to use each
✓ Configure and understand all three access tiers
✓ Create SAS tokens with specific permissions and restrictions
✓ Implement lifecycle management policies with filters
✓ Differentiate between properties and metadata
✓ Enable and configure static website hosting
✓ Implement data protection with versioning and soft delete
✓ Use blob leases for concurrency control
✓ Understand security best practices for blob storage
✓ Know the cost implications of different tiers and operations

---

## Conclusion

Azure Blob Storage is fundamental to the AZ-204 exam and to building cloud applications. The topics we've covered - from basic blob operations to advanced lifecycle management and security features - represent the core knowledge Microsoft expects from Azure developers.

Remember:
- Practice hands-on with the Azure CLI and Portal
- Understand the "why" behind each feature, not just the "how"
- Focus on scenario-based thinking - the exam tests application of knowledge
- Review the Microsoft documentation for each topic
- Take practice exams to identify knowledge gaps

Good luck on your AZ-204 exam! With thorough preparation and hands-on practice, you'll be ready to demonstrate your Azure development expertise.
